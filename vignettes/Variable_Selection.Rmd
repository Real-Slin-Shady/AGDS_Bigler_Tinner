---
title: "Variable Selection"
author: "Nils Tinner"
date: "`r Sys.Date()`"
output: html_document
---

```{r Load the Packages needed, message=FALSE, warning=FALSE, include=FALSE}
# Decide which packages you need. For this Project you need the following:
packages <- c("influxdbclient","ggplot2","tidyverse","lubridate","raster",
              "dplyr","googledrive","caret","rgdal","vip","parsnip",
              "workflows","tune","dials","stringr","terra","stars","sf","plyr",
              "doParallel","terrainr","starsExtra", "pdp", "kableExtra", "recipes","Boruta")

# Load the R script to install and load all the packages from above
source("../R/load_packages.R")
```



```{r Data Wrangling, echo=FALSE, message=FALSE, warning=FALSE}
# We want to know, if a certain file already exists
name.of.file <- "../data/Combined.csv"

# If do not exists such a file, we create it
if (!file.exists(name.of.file)){
  # Load the R script to processing the raw tif files
  source("../R/raw_tif_processing.R")
  
  # Load the R script to create the data frame "combined" which is the basis of this project
  source("../R/data_combination.R")
  
  # We run the loaded function and drop all NAs
  data_combination() 
}#generate file
  combined <- read_csv("../data/Combined.csv") |>
    mutate(temperature = temperature-temp) |>
    drop_na()
  
combined <- dplyr::slice_sample(combined,prop = .01) 

```

```{r Predictors and Recipe, message=FALSE, warning=FALSE, include=FALSE}

# Take all column-names you need as predictors from the combined file
predictors <- combined |>
  dplyr::select(-c(Log_Nr,temperature,timestamp,Name,NORD_CHTOP,OST_CHTOPO,year,month,day,hour,LV_03_E,LV_03_N)) |>
  colnames()


```



```{r Predictors and Recipe, message=FALSE, warning=FALSE, include=FALSE}

set.seed(42)

temperature <- combined$temperature

# run the algorithm
bor <- Boruta::Boruta(
    y = temperature, 
    x = combined[, predictors],
    maxRuns = 50, # Number of iterations. Set to 30 or lower if it takes too long
    num.threads = parallel::detectCores()-1)

# obtain results: a data frame with all variables, ordered by their importance
df_bor <- Boruta::attStats(bor) |> 
  tibble::rownames_to_column() |> 
  dplyr::arrange(dplyr::desc(meanImp))

# plot the importance result  
ggplot2::ggplot(ggplot2::aes(x = reorder(rowname, meanImp), 
                             y = meanImp,
                             fill = decision), 
                data = df_bor) +
  ggplot2::geom_bar(stat = "identity", width = 0.75) + 
  ggplot2::scale_fill_manual(values = c("grey30", "tomato", "grey70")) + 
  ggplot2::labs(
    y = "Variable importance", 
    x = "",
    title = "Variable importance based on Boruta") +
  ggplot2::theme_classic() +
  ggplot2::coord_flip()





# 
# library(glmnet)
# 
# # Fit LASSO regression with cross-validation
# lasso_model_cv <- cv.glmnet(x = as.matrix(combined[, predictors]), y = temperature, alpha = 1, nfolds = 5)
# 
# # Determine the optimal lambda value based on cross-validation
# optimal_lambda <- lasso_model_cv$lambda.min
# 
# # Fit the final LASSO model with the optimal lambda
# final_lasso_model <- glmnet(x= as.matrix(combined[, predictors]), y = temperature, alpha = 1, lambda = optimal_lambda)
# 
# # Get coefficients of the final model
# coefficients <- coef(final_lasso_model)
# 
# # Extract selected predictors based on the final model
# selected_predictors <- as.data.frame(name = row.names(as.matrix(coefficients)),as.matrix(coefficients)) |>
#   filter(s0 != 0)
  


predictors_all <- combined |>
  dplyr::select(-c(Log_Nr,temperature,timestamp,Name,NORD_CHTOP,OST_CHTOPO,year,month,day,hour,LV_03_E,LV_03_N)) |>
  colnames()

predictors_current <- predictors_all


formula_local <- as.formula(paste("temperature","~", paste(predictors_all,collapse  = "+")))

pp <- recipes::recipe(formula_local,
                      data = combined) |>
    recipes::step_YeoJohnson(all_numeric(), -all_outcomes()) |> #extension of BoxCox? we will see wether that works...
    recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
    recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())

#!!!!!! here new implementation
group_folds <- groupKFold(combined$Log_Nr, k = 10)

random_forest <- caret::train(
  pp,
  data = combined,
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(
    method = "cv",
    number = 5,
    index = group_folds,
    savePredictions = "final"
  ),
  tuneGrid = expand.grid(
    .mtry = length(predictors_all)/3,       # default p/3
    .min.node.size = 5,         # set to 5
    .splitrule = "variance"     # default variance
  ),
  # arguments specific to "ranger" method
  replace = FALSE,
  sample.fraction = 0.5,
  num.trees = 100,
  seed = 1982
)

RMSE_OLD <- random_forest$results$RMSE


RMSE_NEW <- RMSE_OLD-0.00001
while(RMSE_OLD >= RMSE_NEW){
  RMSE_OLD <- RMSE_NEW
  RMSE_list <- tibble(name = NULL,RMSE = NULL)
  print("Current RMSE")
  print("----------------")
  print(RMSE_NEW)
  print("----------------")
for (predictor in predictors_current) {
  print("Current calculation")
  print(predictor)
  predictors_temp <- predictors_current[ !predictors_current == predictor]
  formula_local <- as.formula(paste("temperature","~", paste(predictors_temp,collapse  = "+")))
  
# Make a recipe which can be used for the lm, KNN, and Random Forest model
pp <- recipes::recipe(formula_local,
                      data = combined) |>
    recipes::step_YeoJohnson(all_numeric(), -all_outcomes()) |> #extension of BoxCox? we will see wether that works...
    recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
    recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())

  random_forest <- caret::train(
  pp,
  data = combined,
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(
    method = "cv",
    number = 5,
    index = group_folds,
    savePredictions = "final"
  ),
  tuneGrid = expand.grid(
    .mtry = length(predictors_all)/3,       # default p/3
    .min.node.size = 5,         # set to 5
    .splitrule = "variance"     # default variance
  ),
  # arguments specific to "ranger" method
  replace = FALSE,
  sample.fraction = 0.5,
  num.trees = 100,
  seed = 1982
)
  
  print("calculations pass finished")
  RMSE_list <- rbind(RMSE_list,tibble(name = predictor,RMSE = random_forest$results$RMSE))
}
  print("RMSE list of current pass")
  print(RMSE_list)
  RMSE_NEW <- RMSE_list |>
    dplyr::filter(RMSE == min(RMSE))|>
    dplyr::select(RMSE)|>
    as.numeric()
  highest_RMSE <- RMSE_list |>
    dplyr::filter(RMSE == min(RMSE))|> #take away the minimal RMSE when model is missing that parameter then min RMSE
    dplyr::select(name) |>
    as.character()
  
  predictors_current <- predictors_current[! predictors_current %in% highest_RMSE]
  print("Current predictors")
  print(predictors_current)
}





```


