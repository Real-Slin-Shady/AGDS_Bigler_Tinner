---
title: "Workflow to Model the Urban Heat Island Effect of the City of Bern"
author: "Nils Tinner, Patrick Bigler"
date: "12-01-2023"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    latex_engine: xelatex
editor_options:
  markdown:
    wrap: 75
bibliography: references.bib
---

Course: Proseminar in Applied Geo-Data Science at the University of Bern
(Institute of Geography)

Supervisor: Prof. Dr. Benjamin Stocker

Adviser: Dr. Laura Marques, Pepa Arany

Further information: <https://geco-bern.github.io/agds_proseminar/>

[Do you have questions about the workflow? Contact the
authors:]{.underline}

Tinner Nils (nils.tinner\@students.unibe.ch) had the lead for the work
package 1 (WP1):

-   Data wrangling

-   Data (pre)processing

-   Open sciences and reproducible workflow

-   Advanced models (XGB and neuronal network)

Bigler Patrick (patrick.bigler1\@students.unibe.ch) had the lead for the
work package 2 (WP2):

-   Model implementation and calculations

-   Model tuning

-   Model evaluation

-   Visualize the key findings

-   Structure and design of this markdown and the workflow

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

# Introduction^WP1, WP2^

Anthropogenic climate change is expected to increase the amount, intensity,
and duration of heat waves [@burger2021]. The urban heat island (UHI)
effect further amplifies this trend in urban environments [@burger2021];
[@gubler2021]; [@wicki2018]. The UHI effect is expressed by higher air
temperatures in urban areas compared to rural areas in the region
[@oke2017]. The effect is highest during night, as the emission of longwave
radiation in urban environments is impaired and sensible heat fluxes are
enhanced, whilst latent fluxes are reduced [@burger2021], [@gubler2021].
People living in urban areas are thus highly affected by the UHI effect via
thermal stress [@burger2021], [@wicki2018]). Given that more than 75% of
the Central European population lives in urban areas, the increasing trend
poses one of the major weather threats to people in urban environments
[@wicki2018]. Studying spatial temperature variability in urban areas is
therefore crucial to implement adaptation measures to minimize effects on
human health and the environment [@burger2021].

To capture small-scale temperature changes in these climatically complex
areas, high spatial resolution measurement networks are needed. However,
automated weather stations (AWS) are scarce due to their high costs. To
tackle this problem, @gubler2021 developed a new type of low-cost
measurement devices (LCDs). The LCD consists of a temperature logger and a
custom-made radiation shield that is naturally ventilated. 79 LCDs were
installed in the city of Bern, Switzerland in 2018 [@gubler2021]. The study
reported an overestimation of hourly mean temperature measurements by the
LCDs (0.61 °C - 0.93 °C) compared to the reference station (AWS) during
daytime (06:00 -- 22:00). During night-time (22:00 -- 06:00), differences
were much lower or even negative (between -0.12 °C and 0.23 °C). But not
only the LCD temperature and the anomaly between the LCDs and the AWS is
interesting, but also the temperature distribution of the entire region of
Bern, shown on a map [@gubler2021].

In spring 2023, Nils Tinner generated a map of the current LCDs temperature
in the city of Bern based on the study of @burger2021. He improved the map
in June 2023 by also using geospatial data from the study of @burger2021.
These improvements enabled him to generate a map showing the spatial
temperature distribution of the city of Bern. The approach used was a
classic multivariate linear regression model. As the model only knows the
current temperature distribution, its scope and statistical power are
limited. But because both @burger2021 and @gubler2021 showed that the city
of Bern is affected by the urban heat island effect it is crucial to know
more about it. This fact and the limited power of the first map from Nils
Tinner motivated us to investigate the urban heat island once again.

## Objectives and Research Questions

@burger2021 used a multiple linear regression approach to model the urban
heat island effect in the city of Bern for a nighttime mean. In this study
we aim to calculate our own models at an hourly rate all day. Furthermore,
we will implement a classic approach (multiple linear regression model) and
machine learning approaches (k-nearest neighbors (knn) and random forest).
We use two sets of geospatial layers. First, we will use the same set as
@burger2021 with the same zonal mean per predictor class, but since
@burger2021 was selecting layers with nighttime mean modelling in mind this
study also uses generated geospatial layers with several zonal mean
distances per predictor class (these will be introduced in the data
section). Therefore, the following questions arise:

1.  Are the two machine learning approaches (knn and random forest)
    reasonable approaches for modeling the urban heat island effect in the
    city of Bern in terms of explaining the variance ($R^2$), the precision
    (RMSE) and the accuracy (bias)?
2.  Are the machine learning approaches superior to a multiple regression
    model in terms of explaining the variance ($R^2$), the precision (RMSE)
    and the accuracy (bias)?
3.  How do models with the geospatial layers of @burger2021 compare to our
    models containing several zonal means per predictor class in terms of
    explaining the variance ($R^2$), the precision (RMSE) and the accuracy
    (bias)?
4.  Can a meaningful map be generated from the best model that shows the
    spatial distribution of the temperature anomaly in the suburban area of
    the city of Bern?

# Data and Methodology

## Data^WP1^

Temperature data for the years 2019-2022 from the network of the city of
Bern is used. This is a numerical data set of the 3 m temperature in 104
locations, with a temporal resolution of 10 minutes for all LCDs. How many
LCD are active depends on the year but 55 LCDs are active over the entire
time period. The network data is publicly available on BORIS, however since

!!! SHOW SOURCE!!! BORIS, coordinates?! RIP -\> discuss with Patrickone.

data on BORIS is faulty at the moment we uploaded the corrected .csv-files
to the Git- Hub Repo. We use additional data from the AWS at Zollikofen, as
it is the official meteorologic station of Bern. The five meteorological
variables (2m temperature, precipitation, radiation and winddirection and
windspeed) with a temporal resolution of 10 minutes for the years 2019-2022
will be used as well as the timestamp. Data of temperature and
precipitation of the previous six hours and twelve hours, and one, three
and five days and ten days (precipitation only) will also be fed into the
model. This data is uploaded to the repository to ensure availability since
the IDAWEB-data service is open for scientific use but not completely open
access. To show small scale urban patterns of temperature distributions,
geospatial data as shown in @burger2021 is used. These are cantonal land
use classes as well as federal geospatial data. All of the data is
spatially averaged to obtain the layers most effective as shown in
@burger2021. The federal and cantonal data is directly downloaded from the
web into R. Table 1 provides a brief overview of the data we will use.

| Predictor class                                                      | Variable Type    | Zonal Mean Resolution / [3 predictors per class] [@burger2021] |
|-------------------------------|----------------|----------------------------|
| 2m Temperature in °C [Zollikofen] for the time period 2019 - 2022    | Meteorological   | 15 minutes / [1h mean, 6,12,24,72,120 hours rolling mean]      |
| Precipitation in mm [Zollikofen] for the time period 2019 - 2022     | Meteorological   | 15 minutes / [1h mean, 6,12,24,72,120,240 hours rolling sum]   |
| Radiation in $W*m^{-2}$ [Zollikofen] for the time period 2019 - 2022 | Meteorological   | 15 minutes / [1h mean]                                         |
| Windspeed in $m*s^{-1}$ [Zollikofen] for the time period 2019 - 2022 | Meteorological   | 15 minutes / [1h mean]                                         |
| Winddirection in ° [Zollikofen] for the time period 2019 - 2022      | Meteorological   | 15 minutes / [1h mean]                                         |
| Land Cover Building [LC_B]                                           | Geospatial Layer | [25m / 150m / 1000m], Burger used [250m]                       |
| Open Space Forest [OS_FO]                                            | Geospatial Layer | [[25m / 150m / 1000m], Burger used [250m]                      |
| Open Space Garden [OS_GA]                                            | Geospatial Layer | [25m / 150m / 1000m], Burger used [25m]                        |
| Open Space Sealed [OS_SE]                                            | Geospatial Layer | [25m / 150m / 1000m], Burger used [500m]                       |
| Open Space Agriculture [OS_AC]                                       | Geospatial Layer | [25m / 150m / 1000m], Burger used [500m]                       |
| Open Space Water [OS_WA]                                             | Geospatial Layer | 25m / 150m / 1000m], Burger used [150m]                        |
| Vegetation Height [VH]                                               | Geospatial Layer | 25m / 150m / 1000m], Burger used [150m]                        |
| Mean Building Heights [BH]                                           | Geospatial Layer | 25m / 150m / 1000m], Burger used [150m]                        |
| Slope [SLO]                                                          | Geospatial Layer | 25m / 150m / 1000m], Burger used [100m]                        |
| Digital Elevation Model (DEM)                                        | Geospatial Layer | No zonal mean                                                  |
| Aspect [ASP]                                                         | Geospatial Layer | 25m / 150m / 1000m], Burger used [150m]                        |
| Flow accumulation [FLAC]                                             | Geospatial Layer | 25m / 150m / 1000m], Burger used [200m]                        |
| Roughness [ROU]                                                      | Geospatial Layer | [25m / 150m / 1000m], Burger used [25m, replaces SVF]          |
| Climate Network in °C [80 LCDs, different sides] in year 2019        | Meteorological   | 10 minutes / [1h mean]                                         |
| Climate Network in °C [67 LCDs, different sides] in year 2020        | Meteorological   | 10 minutes / [1h mean]                                         |
| Climate Network in °C [67 LCDs, different sides] in year 2021        | Meteorological   | 10 minutes / [1h mean]                                         |
| Climate Network in °C [81 LCDs, different sides] in year 2022        | Meteorological   | 10 minutes / [1h mean]                                         |

: **TABLE 1:** Overview of the predictors used for modeling.

To illustrate the model predictions, we also want to show a spatial
temperature distributions for a possible day as a map as a model output.

## Methodology: Main Goal/Structure

The methodology section is subdivided into preparation and modelling. Our
goal is to predict the temperature anomlay of the on site location to the
official measurement station in Zollikofen and thus the UHI. Therefore the
target variable is the temperature and the predictor variables are a
combination of the above stated meteorologic variables and geospatial
layers. Models are generated later in the workflow and include a linear
regression, a knn and a random forest. Aditionally, a XGBoost and a neural
network can be run.

### Methodology: Preparation^WP1^

In this section, all preparations for modeling are performed.

#### Geospatial layers

Either the geospatial layers are downloaded directly from a remote
repository, which is the case when knitting (default). Alternatively, the
geospatial layers are generated by downloading and processing of the
geospatial layers as raw data from open sources. Processing can be
performed with the layer resolutions by @burger2021 which corresponds to
one predictor per predictor class in table 1. Alternatively and preferably
the geospatial layers are all processed by the resolutions in table 1. This
results in three geospatial layers per predictor class in table 1. The
distances indicated correspond to the performed spatial meaning of the
layer (see @burger2021).

#### Data combination

Data from meteoswiss for the Zollikofen meteostation is averaged to 1 hour
intervals for the variables radiation, windspeed, winddirection, 2m
temperature and precipitation (for precipitation it is summed). Then
rolling means/sums for the 6, 12, 24 hours, 3 days, 5 days are added for
precipitation and temperature (in case of precipitation 10 days as well
beacuse we want to capture the moisture). This is then the processed
version of the meteoswiss meteorologic data. Then data from the loggers in
the city (the LCD temperature loggers) are read in. This is then combined
with the metadata, resulting in a tidy data frame, where each row
corresponds to a measurement of a single logger to a single point in time
with its attributes such as its location as columns (from the metadata).
Then, the values corresponding to each location of the loggers is read out
of the geospatial predictors. This data is also added to the data frame.
This then is the finished Combined.csv with all data.

#### Variable Selection

First, a variable selection was carried out (Markdown::
Variable_Selection.rmd). This was only explored for the predictors in table
1 since layers by @burger2021 were already reduced and selected. The
variable selection with Boruta resulted after removal of variable that
should be removed not due to relevance reasons but other reasons (see table
2), that all variables are relevant. Further variable selection was
explored, but no definitive answers were reached. For that reason, all
generated predictors are used.

Table two provides a brief overview of why certain variables were rejected.
Based on this selection, a formula is now generated, which is processed
into a recipe. This is now the basis for creating the models.

| Variable which is not a predictor | Reason                                                                                                                                                            |
|-----------------|----------------------------------------------------------|
| Log_Nr, name                      | Does not make sens for spatial upscaling                                                                                                                          |
| temperature                       | It is the target                                                                                                                                                  |
| timestamp, year, month, day, hour | Because it is controversial whether time and date should be used as predictors. The model could simply learn the mean of the time and not the underlying pattern. |
| NORD_CHTOP, OST_CHTOP             | Coordinates are not used because because the model should learn the underlying pattern.                                                                           |
| LV_03_E, LV_03_N                  | Coordinates are not used because because the model should learn the underlying pattern.                                                                           |

: **TABLE 2:** Overview about the reasons why a variable/column were
rejected as a predictor

## How this Workflow Works^WP1, WP2^

In the Data and Methodology section you will see that we work with a large
amount of data. This means the computational time could be very high. To
address this issue we implemented different workflows within this markdown.
The file runs automatically in the most performant mode when knitting. When
run normally, the terminal will guide you through the decision steps about
processing. You can also change the way this files knits by changing the
parameters manually in the knitting settings blow. Figure 1 gives you a
overview about the default mode and all other options you have:

SCHEMATIC OF THE DIFFERENT WORKFLOWS

Please note that the default version is only guaranteed until the end of
February 2024. After that, geospatial data processing that is dependent on
the VH of WSL (Eidg. Forschungsanstalt für Wald, Schnee und Landschaft)
will not be executed anymore and thus miss as predictor layers. This will
alter the results.

### Workflow

This subsection generate your basic .csv file (it is called 'Combined') in
a interactive way when not knitting. Therefore, you have to navigate
through some questions. There are 3 main options when running the scrip:

#### Geospatial layers

The geospatial layers vary in this workflow. As described above, this
script can run either all geospatial layers with three different zonal mean
values or with the zonal mean values as demonstrated by @burger (table1).
This can be chosen in the console when run normally. Because the generation
of the zonal means of the geospatial layers is very time-intensive (20min -
2h, depends on your device), when knitting, the files are always downloaded
from an external source (Dropbox). When the files are downloaded. The
version with all geospatial layers is downloaded. When run not knitted, the
console will ask whether to download, or generate the geospatial means, as
well as which geospatial means.

#### Data size

Computation of models is also very time-intensive. For this reason, there
is the option to run the model in "demo mode", i.e. just using 5% of the
data. This data is randomly picked and then the model is run. This will
substantially worsen the models but is preferred for knitting because of
processing time. This can be changed when knitting with setting the
variable model_demo \<- "n". When run not knitted, the console will ask
whether to run with 5% of the data or not.

#### Advanced models

There are two advanced models: one is a xgBoost model and one a improvised
neural network. These are were not part of the proposal and are thus
considered secondary information. If these should be run while knitting,
the variable advanced_models \<- "y". When run not knitted, the console
will ask whether to run them. It is strongly recommended to only run these
with 5% of the data!

### Packages

This code chunk install and load all packages you need to reproduce this
project. If you think you need another package as well, then write it into
the vector 'packages' and run the code again.

```{r Load the Packages needed, echo=TRUE, message=FALSE, warning=FALSE}
# Decide which packages you need. For this Project you need the following:
packages <- c("influxdbclient","ggplot2","tidyverse","lubridate","raster",
              "dplyr","googledrive","caret","rgdal","keras","vip","parsnip",
              "workflows","tune","dials","stringr","terra","stars","sf","plyr",
              "doParallel", "foreach", "terrainr","starsExtra", "pdp", "recipes", 
              "tidyterra","shiny", "xgboost", 'kableExtra','rnaturalearth','zoo',
              'moments', 'tibble')
```

```{r Choose your workflow mode, message=FALSE, warning=FALSE, include=FALSE}
# Set seed for reproducibility
set.seed(123)
# Load the R script to install and load all the packages from above
source("../R/load_packages.R")  
load_packages(packages = packages)

if (isTRUE(getOption('knitr.in.progress'))) { 
  ### For knitting, adjust here! ###
  # Model_demo means, you only keep 5% of the data for training
  model_demo <- "y" 
  # Advanced models are run if set to "y"
  advanced_models <- "n" 

# Directly download the data
source("../R/demo_download.R")
# and generate the .csv file
source("../R/data_combination.R")
data_combination()
}else{##runs as normal R-Script:
# Start the brains. They will guide you through the questions to change the workflow
source("../R/Processing_Brain.R")
preprocessing()

source("../R/model_training_brain.R")
model_training_brain()
}

# Read your .csv file
combined <- read_csv("../data/Combined.csv") |>
  mutate(temperature = temperature-temp) |>
  drop_na()
  # Choose 5% of the data randomly for the demo mode
  if (model_demo == "y") {
    combined <- dplyr::slice_sample(combined,prop = .05) 
  }

# Random sample
loggers_test <- sample(unique(combined$Log_Nr), 10)

# Generate a test set
combined_test <- combined |> 
  filter((Log_Nr %in% loggers_test))

# Generate a training set
combined_train <- combined |> 
  filter(!(Log_Nr %in% loggers_test))
```

### Model Recipe

The model recipe corresponds to the defined predictors minus the in table 2
mentioned removed variables. Furthermore a step_YeoJohnson (includes Box
Cox and an extansion. Now, it can handle x ≤ 0), step_center (subsracting
the mean from each observation/measurement), and a step_scale (transforming
numeric variables to a similar scale) are preformed.

```{r Predictors and Recipe, echo=FALSE, message=FALSE, warning=FALSE}
# Take all column-names you need as predictors from the combined file
predictors <- combined |>
  # select our predictors (we want all columns except those in the select() function)
  dplyr::select(-c(Log_Nr,temperature,timestamp,Name,NORD_CHTOP,OST_CHTOPO,
                   year,month,day,hour,LV_03_E,LV_03_N)) |>
  colnames()

# Define a formula in the following format: target ~ predictor_1 + ... + predictor_n
formula_local <- as.formula(paste("temperature","~", paste(predictors,collapse  = "+")))
  
# Make a recipe which can be used for the lm, KNN, and Random Forest model
pp <- recipes::recipe(formula_local,
                      data = combined_train) |>
    # Yeo-Johnsen transformation (includes Box Cox and an extansion. Now it can handle x ≤ 0)
    recipes::step_YeoJohnson(all_numeric(), -all_outcomes()) |> 
    # subsracting the mean from each observation/measurement
    recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
    # transforming numeric variables to a similar scale
    recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())
```

## Methodology: Modelling^WP2^

In this methodology subsection, we describe how the models are implemented
and specify the hyperparameters that should be used. It is important to
note that we have already conducted hyperparameter tuning, but exclusively
for the regular mode. Consequently, if you choose to use the demo mode (and
thus for knitting), the models may be under- or overfitted and may not
perform well. As mentioned before, we distinguish between our basic models
and our advanced models.

### Basic Models implementation

The basic models include linear regression, k-nearest neighbors (knn), and
the random forest model. We will compare their performance based on their
ability to explain variance, accuracy, and precision in predicting the
target. In the following subsections, we will introduce you to these
models. Please note that we utilize DoParallel due to the extensive size of
our data set. Additionally, the models are designed and tuned to deliver
optimal performance in regular mode. Consequently, significant deviations
are inevitable when operating in demo mode exclusively.

#### Implementation of the Linear Regression Model

The lm-model is a classic multiple linear regression model. Calculating the
model requires a recipe and training data as inputs, and there is no option
available for model tuning. To enhance model performance, three
cross-validations are performed by logger number. The computational time
should not exceed 10 seconds for the demo mode and 60 seconds for the
regular mode (depending on your device).

```{r Calculate lm-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to calculate a lm model
source("../R/lm_model.R")

# The function needs the recipe and a dataset which can be used for model training
lm_model <- LM_Model(pp, combined_train)
```

#### Implementation of the KNN Model

Calculating the KNN model requires a recipe and training data as inputs.
The function computes a KNN model with the recipe and employs 3
cross-validations by logger numbers. By default, the hyperparameter "k" is
set to 10. In demo mode, model tuning is not meaningful because it exists
for demonstration purposes only. The estimated computational time is about
25 seconds.

The function has a safety feature implemented because the computing time
for a KNN model in regular mode can be very high. In the worst case, one
must compute $O(cv⋅n⋅d+cv⋅k⋅n)$ distances, where "n" is the number of
observations, "d" is the number of dimensions, "cv" is the number of
cross-validations, and "k" is the hyperparameter. For this reason, a
typical device may require several hours. The safety feature limits the
data input to 100,000 rows. The adjustment takes place automatically, and
you will be informed about how many rows are removed and what percentage of
the original data remains. Note that model tuning takes place for the
regular mode.

KNN utilizes "k" as a hyperparameter, providing an option for model tuning.
When setting "tuning=TRUE", the function automatically fine-tunes the model
for the specified "k" values (k=c(8:12)). Since the determination of the
hyperparameter "k" is unique, you can consider the tuning successful if "k"
belongs to $k\in[9, 10, 11]$. In cases where "k" is either 8 or 12, we
recommend accessing the corresponding R script and expanding the range of
the "tuning_vector". The default setting is "k=10".

coputationa time abou 5min

```{r Calculation of the knn-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to calculate a KNN model
source("../R/knn_model.R")

# The function needs the recipe and a dataset which can be used for model training
knn_model <- KNN_Model(pp,combined_train, tuning = FALSE)
```

#### Implementation of the Random Forest Model

Calculating the random forest model requires a recipe and training data as
inputs. The function computes a random forest model with the recipe and
employs 3 cross-validations using logger numbers. The model requires two
hyperparameters. By default, the 'mtry' hyperparameter is set to 16 (number
of variables in the recipe divided by 3.5). This hyperparameter represents
the number of features randomly sampled at each split when constructing a
decision tree within the forest, controlling the diversity among the trees
in the ensemble. The second one is 'min.node.size,' which controls the
minimum number of data points required to create a node in a decision tree
within the forest, helping to control the size of the tree and prevent
overfitting. By default, this hyperparameter is set to 3.

In demo mode, model tuning is not meaningful because it exists for
demonstration purposes only. The estimated computational time to generate a
random forest model is about 15 seconds. In regular mode, we provide an
option for fine-tuning. If you wish to perform fine-tuning, change the
tuning option to TRUE. The estimated computational time is about 100
seconds.

```{r Calculation of the random forest-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to calculate a random forest model
source("../R/random_forest.R")

# The function needs the recipe and a dataset which can be used for model training
random_forest <- random_forest(pp, combined_train, tuning = FALSE)
```

### Advanced Models implementation^WP1^

#### Implementation of the XGBoost

The XGBoost-model can be time intensive. We use the DoParallel Package for
that. Please make sure, that your device does nothing else. \####
Implementation of the XGB Model

All hyperparameters are tuned, cross validation is performed for 3 folds by
the logger ID.

```{r}

if(advanced_models == "y"){
source("../R/XGB.R")
# type of task we want to evaluate

#this model always tunes...
xgb_model <- xgb(data_train = combined_train,formula = formula_local) }
```

#### Implementation of the neuronal network

A neural network with 223 neurons is used. learning rate is set to 0.001
and a batch size of 64. These are some standard values for the data size of
5% of the data.

```{r}

if(advanced_models == "y"){
  source("../R/neural_network.R")
  #tensorflow::install_tensorflow() may need to be run...
  nn_model <- neural_network(combined_train)
#preprocessing needed for test imput of nn
  predictors_nn <- combined_test |>
  dplyr::select(all_of(predictors))
temperature <- combined_test |>
  dplyr::select(temperature)

# Normalize/Scale the predictors using dplyr
scaled_predictors <- predictors_nn|>
  mutate(across(everything(), scale))

# Combining the scaled predictors with the temperature into a single data frame
processed_data <- cbind(scaled_predictors, temperature)


test_data <- processed_data

  
test_features <- test_data|> select(-temperature)
test_labels <- test_data|> pull(temperature)
}  
```

# Results

In this section, we present our findings in terms of the explanation of
variance ($R^2$), precision (RMSE), and accuracy (bias). Once again, we
distinguish between basic models (lm, knn, and random forest models) and
advanced models (XGBoost and neural network). An overview of the results
can be found in tables 3-5 in the Summary of Key Findings subsection.

## Basic Models^WP2^

This subsection evaluates the basic models. To ensure comparability, the
basic models are assessed using the same evaluation function. The function
takes a training data set, a test data set, and the model as inputs. The
mode in which the respective model was generated is irrelevant, as the
function processes them identically. However, it should be noted that all
hyperparameters were tuned for the regular mode during the implementation
of the models. Consequently, the quality of the models varies significantly
depending on the mode.

The function returns a list containing four elements:

1.  The first element is a table that provides an overview of the metrics
    (R2R2, RMSE, MAE, and bias) for the model itself, including evaluations
    on both the training set and the test set (which should be considered)

2.  The second element

3.  The thirdelement is a visualization of the model's performance on both
    the training set and the test set. This includes key metrics, providing
    an overview of how the model performed and where its performance is
    optimal or suboptimal. The orange line represents the ideal
    performance, and if all points lie on this line, the model predicts the
    target perfectly. The red line depicts the actual regression, and the
    closer it is to the orange line, the better the model.

4.  The fourth element is a boxplot for the LCDs used for evaluation in the
    test set. This allows you to see which LCDs are being used and how they
    performed.

5.  The fivth element is a boxplot for the hour of the day. This plot shows
    you the deviation at different times of the day.

Note that this study aims to find the best model with the available data.
Therefore, the evaluation has been conducted in the regular mode. The
results described in the evaluation subsections cannot be transferred to
other modes. If you are interested in the findings of the demo mode or the
\@burger2021 mode, please refer to Table 3 and 4, respectively.

### Evaluation of the lm-model

Here, we evaluate the lm-model using the evaluation function. Computational
time should not be a concern, and we estimate it to be about 30 seconds.

The first element is a table that shows the main metrics. The multiple
linear regression model explains 45% of the variance. Furthermore, it
exhibits a precision (RMSE) of approximately 0.88°C and an accuracy (bias)
of around 0.09°C. The bias of the training data is zero, indicating a
successful implementation of the evaluation function, as required by
definition.

The second element displays the main statistical parameters. This reveals a
relatively high spread of about 12.5 °C. However, the IQR is approximately
1.1°C, signifying that 50% of the data falls within this interval. The
skewness is negative, indicating a longer right tail and an asymmetric
distribution. We conclude that the model contains negative outliers (mean
\< median). This conclusion is confirmed by boxplots (fifth element).

The third element is the visualization, which reveals that the model does
not perform well at high temperatures. The plot also displays negative
outliers (note dots in the test set evaluation plot, bottom right).

The fourth element comprises boxplots by logger side. All ten box-whiskers
are within a ±2 °C range, but there are outliers, especially in Murifeld
(LCD 58). Otherwise, the performance is relatively stable, even when
different geospatial layers dominate (water (Egelsee, LCD 12), forest
(Bremgartenwald, LCD 32), and old town (Bundesplatz, LCD 70)).

The fifth element is the evaluation by the hour of the day. Between 21:00 -
7:00 (night-time), the overestimation is relatively constant and
corresponds to the findings of \@burger2021. Between 6:00 - 8:00 and
18:00 - 20:00, there are two very similar peaks in overestimation. Between
those peaks, the model underestimates the temperature in the city of Bern
(day-time). The plot also indicates that outliers tend to be negative,
which is explained by the negative skewness.

```{r Evaluation of the lm-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to evaluate the model
source("../R/evaluation.R")
# The function will return a list
evaluation <- evaluation_function(combined_test, combined_train, lm_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# Overview about the main statistics
evaluation[[2]]
# If we want a visualization of the training and test set
evaluation[[3]]
# Boxplot for each logger station 
evaluation[[4]]
# Boxplot for each hour of the day (24h)
evaluation[[5]]
```

### Evaluation of the knn-model

Here, we evaluate the knn-model using the evaluation function.
Computational time can be a concern, and we estimate it to be about 5
minutes.

The first element is a table that displays the main metrics. The knn model
explains 48% of the variance. Furthermore, it demonstrates a precision
(RMSE) of approximately 0.86°C and an accuracy (bias) of around 0.04°C.

The second element reveals the main statistical parameters. This shows a
spread of about 12.1 °C. The IQR is approximately 1.01°C, containing 50% of
the data. The skewness is slightly negative, indicating that the
distribution is almost normal. We conclude that the model contains a
balanced amount of negative and positive outliers (mean ≈ median). This
conclusion is further supported by boxplots (fifth element).

The third element is the visualization. It illustrates that the predicted
values underestimate the real values.

The fourth element consists of boxplots by logger side. It appears that the
IQR for each box is relatively stable. However, the whiskers vary (small
for Bremgartenwald and high for Weyermannshaus) but are within a ±3 °C
range. Once again, Murifeld (LCD 58) exhibits the most outliers. Otherwise,
the performance is relatively stable.

The fifth element is the evaluation by the hour of the day. The first
notable observation is the large spread, as described in the main
statistical parameters. Between 22:00 - 8:00 (night-time), the
overestimation is relatively constant, aligning with the findings of
@burger2021. Between 10:00 and 17:00, the model underestimates the target.
The plot also suggests that outliers tend to be balanced, indicating that
the model does not prefer a specific direction.

```{r Evaluation of the knn-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to evaluate the model
source("../R/evaluation.R")

# Function call
evaluation <- evaluation_function(combined_test, combined_train, knn_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# Overview about the main statistics
evaluation[[2]]
# If we want a visualization of the training and test set
evaluation[[3]]
# Boxplot for each logger station 
evaluation[[4]]
# Boxplot for each hour of the day (24h)
evaluation[[5]]
```

### Evaluation of the random forest-model

Here, we evaluate the knn-model using the evaluation function.
Computational time can be a concern, and we estimate it to be about 60
seconds.

The first element is a table that shows the main metrics. The random forest
model explains 73% of the variance. The RMSE (precision) is about 0.62°C,
and the bias (accuracy) is about 0.08°C, indicating that, in general, the
model slightly overestimates the target.

The second element is a table which contains the main statistic parameters.
The spread is about 7.4°C. The IQR is about 0.64°C. The skewness is
positive which means the left tail is longer. This indicates that outliers
are tend to be positive. This conclusion is further supported by boxplots
(fifth element).

The third element is the visualization, revealing that the model performs
well at high temperatures but struggles to deal with low-temperature
anomalies.

The forth element consists of boxplots by logger side. All ten box-whiskers
are within a ±2 °C range, but each one shows outliers, especially in
Murifeld (LCD 58). Otherwise, the performance is relatively stable, even
when different geospatial layers dominate (water (Egelsee, LCD 12), forest
(Bremgartenwald, LCD 32), and old town (Bundesplatz, LCD 70)).

The fifth element is the evaluation by the hour of the day. Between 21:00 -
7:00 (night-time), the overestimation is relatively constant and
corresponds with the findings of \@burger2021. During day-time (9:00 and
18:00), the model underestimates the temperature in the city of Bern.
Furthermore, the spread is about 7°C and tends to be larger in
underestimating than in overestimating, especially during night-time.

```{r Evaluation of the random forest-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to evaluate the model
source("../R/evaluation.R")

# Function call
evaluation <- evaluation_function(combined_test, combined_train, random_forest)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# Overview about the main statistics
evaluation[[2]]
# If we want a visualization of the training and test set
evaluation[[3]]
# Boxplot for each logger station 
evaluation[[4]]
# Boxplot for each hour of the day (24h)
evaluation[[5]]
```

## Advanced Models^WP1^

### Evaluation of the xgb-model

```{r Evaluation of the XGB boost, echo=FALSE, message=FALSE, warning=FALSE}

if(advanced_models == "y"){

source("../R/evaluation.R")

# Function call
evaluation <- evaluation_function(combined_test, combined_train, xgb_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# Overview about the main statistics
evaluation[[2]]
# If we want a visualization of the training and test set
evaluation[[3]]
# Boxplot for each logger station 
evaluation[[4]]
# Boxplot for each hour of the day (24h)
evaluation[[5]]
}
```

### Evaluation of the neuronal network

The evaluation is done manually since the normal evaluation function is not
compatible with the neural network.

```{r Evaluation of the neuronal network, echo=FALSE, message=FALSE, warning=FALSE}

if(advanced_models == "y"){  
  # Evaluate the model
evaluation <- nn_model|> evaluate(as.matrix(test_features), test_labels)
cat("Mean Absolute Error on Test Data:", evaluation, "\n")

# Make predictions
combined_test$nn_prediction <- nn_model|> predict(as.matrix(test_features))


print(paste0("RSQ: ",cor(combined_test$nn_prediction,combined_test$temperature)^2))
print(paste0("RMSE:",RMSE(combined_test$nn_prediction,combined_test$temperature)))
print(MAE(paste0("MAE: ",combined_test$nn_prediction,combined_test$temperature)))
print(paste0("BIAS: ",mean(combined_test$nn_prediction-combined_test$temperature)))
}
```

## NEW HEADER

Since random forest performes the best, we now look at the impact of
variables. The generation of these plots takes very long even with little
data. therefore only the two most important (based on boruta) meteorologic
(top) and geospatial (bottom) variables are plotted.

```{r Evaluation of the random forest-model, echo=FALSE, message=FALSE, warning=FALSE}

if(model_demo == "y"){ #only when in demo model, to computational expensive
# The predictor variables are saved in our model's recipe
preds <- 
  random_forest_model$recipe$var_info |> 
  dplyr::filter(role == "predictor") |> 
  filter(variable %in% c("rad","winds",
         "OS_SE_150","OS_GA_150"))|>
  dplyr::pull(variable) 

# The partial() function can take n=3 predictors at max and will try to create
# a n-dimensional visulaisation to show interactive effects. However, 
# this is computational intensive, so we only look at the simple 
# response-predictor plots
all_plots <- purrr::map(
  preds, #we take the 3 most important spatial layers and the 3 most important meteorologic layers from variable selection.
  ~pdp::partial(
      random_forest_model,       # Model to use
      .,            # Predictor to assess
      plot = TRUE,  # Whether output should be a plot or dataframe
      plot.engine = "ggplot2"  # to return ggplot objects
    ), .progress = T
)

cowplot::plot_grid(plotlist = all_plots, ncol = 2) 

} y
```

## Summary of the Key Findings^WP2^

### Overview about the Evaluation of the Models

VALUES ARE OK (11.12.2023)!!

| Model             | RSQ [Test set] | RMSE [Test set] | MAE [Test set] | Bias [Test set] |
|---------------|---------------|---------------|---------------|---------------|
| Linear Regression | 0.455          | 0.899           | 0.711          | 0.102           |
| KNN               | 0.542          | 0.824           | 0.628          | 0.093           |
| Random Forest     | 0.522          | 0.846           | 0.642          | 0.48            |
| XGB Boost         | \-             | \-              | \-             | \-              |
| Neuronal Network  | \-             | \-              | \-             | \-              |

: **TABLE 4:** Overview of the evaluation and results of the models with
data from Burger et al. (2019) with 5% of the data.

To fully implement the Open Sciences approach, the models can be computed
using the following code chunks. Further, for every model, a short synopsis
of the optimized hyperparameters is given. It must therefore be decided
before running the code chunk whether the hyperparameters determined by us
are to be used or whether the hyperparameters are to be optimized.

| Element of the list | What do you find                                                                               |
|-----------------|----------------------------------------------------------|
| 1                   | returns a table with the main metrics (RSQ, RMSE, MAE and Bias)                                |
| 2                   | retunrs a visualization of the model (Training and Test set) and shows RSQ, RMSE and the bias. |
| 3                   | returns a boxplot for each logger station (24h)                                                |
| 4                   | returns a boxplot for each hour of the day                                                     |

: **TABLE 2:** Overview of the returned list of the evaluation function

The aim of this project is the comparison of machine learning techniques to
a linear regression. Further, we want to explore the superior of machine
learning compared to a linear regression. Table 3 gives you an overview
about our results, if you use the entire 'combined' data set. You see, that
both KNN and random forest are superior compared to the linear regression
model.

The random forest model explains about 73% of the variance. The RSME is
about 0.63 °C. Note that the error of a single LCD in the climate network
of the city of Bern is about 1.5°C (see Burger et all 2019). Further we can
see, that data scattering is about 0.2°C lower than for a KNN- or lm-model
(accuracy). The bias is for all models about 0.07°C (systematic error /
precision). It is positive and therefore our models overestimate the
temperature (see evaluation and access the second element of the list).

The XGB Boost and the neuronal network are additional and should show you,
that there are further techniques which could be explored. But those model
we did not taken into account.

VALUES ARE OK (11.12.2023)!!

| Model                | RSQ [Test set] | RMSE [Test set] | MAE [Test set] | Bias [Test set] |
|---------------|---------------|---------------|---------------|---------------|
| Linear Regression    | 0.45           | 0.88 °C         | 0.69 °C        | 0.088 °C        |
| KNN [reduced to 13%] | 0.48           | 0.86 °C         | 0.65 °C        | 0.04 C °        |
| Random Forest        | 0.73           | 0.62 °C         | 0.46 °C        | 0.083 °C        |
| XGB Boost            | \-             | \-              | \-             | \-              |
| Neuronal Network     | \-             | \-              | \-             | \-              |

: **TABLE 3:** Overview of the evaluation of the models in the regular
mode.

As mentioned, the model calculation can be very time intensive. Table 4
shows the results for approach with reduced data. Overall we can say, that
the models perform less. All values are lesser than in table 3. Note the
high bias for the random forest. The model highli overestimate the
temperature. We do not have an explanation for this. May be there is a
problem with the reducing process. If we only use 5% of our data, then we
lose a lot of information. If we lose all information about a certain
logger station but the model predict this station, the it could b that the
model perfoms worse. But we are not sure and more investigations are
needed. But as a conclusion we can say, that more data improve the models.

VALUES ARE OK (11.12.2023)!!

| Model             | RSQ [Test set] | RMSE [Test set] | MAE [Test set] | Bias [Test set] |
|---------------|---------------|---------------|---------------|---------------|
| Linear Regression | 0.422          | 0.93 °C         | 0.739 °C       | 0.125 °C        |
| KNN               | 0.457          | 0.91 °C         | 0.704 °C       | 0.05 °C         |
| Random Forest     | 0.477          | 0.899 °C        | 0.661 °C       | 0.184 °C        |
| XGB Boost         | \-             | \-              | \-             | \-              |
| Neuronal Network  | \-             | \-              | \-             | \-              |

: **TABLE 4:** Overview of the evaluation of the models with 5 % of the
data.

## Map^WP2^

This subsections aims to generate maps to visualize our key findings. For
this purpose we defined a default-vector. This vector shows the numeric
default values for each variable (see table x). Further, we defined
reasonable intervals for each variables. Because we did not train our
models with extrem values (because we do not have a lot) the model perform
worse with extreme values. That is why we took the hihest values from
MeteoSchweiz and reduced it by about 10%. We recommend no to work with
those extreme values. Moreover, the values should be realistic. For example
it is unrealistic if you set your 6h temperature mean to 20°C and your 12h
mean to -5°C. Please consider this if you play around.

| Variable                | Default           | Interval                                             |
|------------------|------------------|---------------------------------------|
| Temperature             | 22 °C             | $x[°C] \in[-15 \leq x \leq 35]$                      |
| Rain                    | 5 mm              | $x[mm] \in[0 \leq x \leq 200]$                       |
| Radiation               | 290 $W*m^{-2}$    | $x[W*m^{-2}] \in[0 \leq x \leq 300]$, for a 10' mean |
| Windspeed               | 8 $m*s^{-1}$      | $x[m*s^{-1}] \in[0 \leq x \leq 20]$                  |
| Winddirection           | 200 ° (west wind) | $x[°] \in[0 \leq x \leq 360]$, with 0° = N, 180° = S |
| 6h mean temperature     | 22                | $x[°C] \in[-15 \leq x \leq 35]$                      |
| 12h mean temperature    | 19                | $x[°C] \in[-15 \leq x \leq 35]$                      |
| 1d mean temperature     | 20                | $x[°C] \in[-15 \leq x \leq 35]$                      |
| 3d mean temperature     | 18                | $x[°C] \in[-15 \leq x \leq 35]$                      |
| 5d temperature          | 17                | $x[°C] \in[-15 \leq x \leq 35]$                      |
| 6h precipitation [sum]  | 0                 | $x[mm] \in[0 \leq x \leq 100]$                       |
| 12h precipitation [sum] | 0                 | $x[mm] \in[0 \leq x \leq 200]$                       |
| 1d precipitation [sum]  | 20 mm             | $x[mm] \in[0 \leq x \leq 400]$                       |
| 3d precipitation [sum]  | 15 mm             | $x[mm] \in[0 \leq x \leq 600]$                       |
| 5d precipitation [sum]  | 10 mm             | $x[mm] \in[0 \leq x \leq 600]$                       |
| 10d precipitation [sum] | 5 mm              | $x[mm] \in[0 \leq x \leq 600]$                       |

: **Table x:** Overview about the default values for the variables used to
predict the map. Further, it shows reasonable possible values for the
variables (SOURCE: METEOSCHWEIZ).

# Spatial Upscaling^WP2^

```{r Generate a map, echo=FALSE, message=FALSE, warning=FALSE}

# Load the function to generate the map
source('../R/map_generator.R')

#scenario default, rainy stormy day
default = c(22, 5, 290, 8, 200, 22, 19, 20, 18, 17, 0, 0, 20, 15, 10, 5)

# Define a vector with your inputs (here, we use a default as described in table x)
#scenario heat day evening
normal_summarday = c(26, 0, 0, 1, 180, 30, 28 , 24, 22, 22, 0, 0, 0, 0, 0, 0)

cold_rain = c(26, 0, 0, 1, 180, 30, 28 , 24, 22, 22, 0, 0, 0, 0, 0, 0)

heat_cloudless = c(26, 0, 0, 1, 180, 30, 28 , 24, 22, 22, 0, 0, 0, 0, 0, 0)

# Function call
map_generater(model = random_forest_model, meteoswiss_input = default)
```

# Discussion

## Data^WP1^

## Methodology

More data quality

KNN is not suitable for large data sets because of the high computational
time

## Conclusion^WP2^

# Bibliography^WP2^
