---
title: "main"
author: "Nils Tinner, Patrick Bigler"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
  '': default
editor_options:
  markdown:
    wrap: 75
---

## Preparation

First, we need some R-Packages. This code chunk install and load all
packages you need for reproduce this project. If you think you need another
package as well, then write it into the vector 'packages' and run the code
again.

```{r Load the Packages needed, message=FALSE, warning=FALSE, include=FALSE}
# Decide which packages you need. For this Project you need the following:
packages <- c("influxdbclient","ggplot2","tidyverse","lubridate","raster",
              "dplyr","googledrive","caret","rgdal","keras","vip","parsnip",
              "workflows","tune","dials","stringr","terra","stars","sf","plyr",
              "doParallel","terrainr","starsExtra", "pdp", "kableExtra", "recipes",
              "tidyterra","shiny")

# Load the R script to install and load all the packages from above
source("../R/load_packages.R")
```

## Data Wrangling

In this section, the data...

### Raw TIF processing

What are we doing here and why?

### Data Combination

This code chunk creates the 'combined.csv' file which is the basis of the
whole project. It contains 10 geospatial data (6 land use classes and 4
geospatial layers)

```{r Data Wrangling, echo=FALSE, message=FALSE, warning=FALSE}

source("../R/Processing_Brain.R")

  repeat{ print("The normal version of model Training takes a while.")
  model_demo = readline(prompt = "Would you like to run the model training in demo mode? [y/n] ")
  if(model_demo %in% c("y","n")){break}
  }

  combined <- read_csv("../data/Combined.csv") |>
    mutate(temperature = temperature-temp) |>
    drop_na()



  if (model_demo == "y") {
    combined <- dplyr::slice_sample(combined,prop = .05) 
  }
  
  set.seed(123)
  loggers_test <- sample(unique(combined$Log_Nr), 10)

  combined_test <- combined |> 
    filter((Log_Nr %in% loggers_test))
  combined_train <- combined |> 
    filter(!(Log_Nr %in% loggers_test))
```

# Models

In this section, we define our predictors

## Model Definitions and Recipe

```{r Predictors and Recipe, message=FALSE, warning=FALSE, include=FALSE}

# Take all column-names you need as predictors from the combined file
predictors <- combined |>
  dplyr::select(-c(Log_Nr,temperature,timestamp,Name,NORD_CHTOP,OST_CHTOPO,year,month,day,hour,LV_03_E,LV_03_N)) |>
  colnames()

# Define a formula in the following format: target ~ predictor_1 + ... + predictor_n
formula_local <- as.formula(paste("temperature","~", paste(predictors,collapse  = "+")))
  
# Make a recipe which can be used for the lm, KNN, and Random Forest model
pp <- recipes::recipe(formula_local,
                      data = combined_train) |>
    recipes::step_YeoJohnson(all_numeric(), -all_outcomes()) |> #extension of BoxCox? we will see wether that works...
    recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
    recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())
```

# Different Models

Here, you can select the model that will be used to eventually estimate the
map's spatial temperature distribution. You will find that the model with
the random forest approach shows the best performance, which is why it was
used for the map.

## Evaluation of the models

To fully implement the Open Sciences approach, the models can be computed
using the following code chunks. Further, for every model, a short synopsis
of the optimized hyperparameters is given. It must therefore be decided
before running the code chunk whether the hyperparameters determined by us
are to be used or whether the hyperparameters are to be optimized.

| Element of the list | What do you find                            |
|---------------------|---------------------------------------------|
| 1                   | A metric table (RSQ, RMSE, MAE and Bias)    |
| 2                   | Model visualization (Training and Test set) |
| 3                   | Boxplot for each logger station (24h)       |
| 4                   | Boxplot for each hour of the day            |
| 5                   | Partial dependence of the variables         |

: **TABLE 1:** Overview of the function return of the evaluation function

dedd

| Model             | RSQ [Test set] | RMSE [Test set] | MAE [Test set] | Bias [Test set] |
|---------------|---------------|---------------|---------------|---------------|
| Linear Regression |                |                 |                |                 |
| KNN               |                |                 |                |                 |
| Random Forest     |                |                 |                |                 |
| XGB Boost         |                |                 |                |                 |
| Neuronal Network  |                |                 |                |                 |

: **TABLE 2:** Overview of the evaluation of the models in the regular
mode.

dedferfef

| Model             | RSQ [Test set] | RMSE [Test set] | MAE [Test set] | Bias [Test set] |
|---------------|---------------|---------------|---------------|---------------|
| Linear Regression | 0.25           | 0.96 °C         | 0.76 °C        | -0.07 °C        |
| KNN               | 0.41           | 0.86 °C         | 0.66 °C        | 0.092 °C        |
| Random Forest     | 0.61           | 0.7 °C          | 0.53 °C        | 0.086 °C        |
| XGB Boost         |                |                 |                |                 |
| Neuronal Network  |                |                 |                |                 |

: **TABLE 3:** Overview of the evaluation of the models in the demo mode.

## LM Model

```{r Calculate a LM-Model, message=FALSE, warning=FALSE}
# Load the R script to calculate a lm model
source("../R/lm_model.R")
# The function needs the recipe and a dataset which can be used for model training
lm_model <- LM_Model(pp, combined_train)


source("../R/evaluation.R")
# The function will return a list
evaluation <- evaluation_function(combined_test, combined_train, lm_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# If we want a visualization of the training and test set
evaluation[[2]]
# Boxplot for each logger station (24h)
evaluation[[3]]
# Boxplot for each hour of the day
evaluation[[4]]
```

## KNN Model

hyperparamter = 10

```{r KNN Model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to calculate a KNN model
source("../R/knn_model.R")

# The function needs the recipe and a dataset which can be used for model training
knn_model <- KNN_Model(pp = pp, training_data =  combined_train, tuning = FALSE)

source("../R/evaluation.R")
evaluation <- evaluation_function(combined_test, combined_train, knn_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# If we want a visualization of the training and test set
evaluation[[2]]
# Boxplot for each logger station (24h)
evaluation[[3]]
# Boxplot for each hour of the day
evaluation[[4]]
```

## Random Forest Model

Hyperparamater:

```{r Random Forest, echo=FALSE, message=FALSE, warning=FALSE}
source("../R/random_forest.R")
random_forest_model <- random_forest(pp,combined_train,tuning = F)

source("../R/evaluation.R")
evaluation <- evaluation_function(combined_test, combined_train, random_forest_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# If we want a visualization of the training and test set
evaluation[[2]]
# Boxplot for each logger station (24h)
evaluation[[3]]
# Boxplot for each hour of the day
evaluation[[4]]
```

## XGB Model

Hyperparamater:

```{r}

source("../R/XGB.R")
# type of task we want to evaluate
model_settings <- parsnip::boost_tree(
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  stop_iter = 20,
) |>
  set_engine("xgboost",nthread = 6, verbose = T) |>
  set_mode("regression")
xgb_workflow <- workflows::workflow() |>
  add_formula(formula_local) |>
  add_model(model_settings)



xgb_results <- xgb(xgb_workflow,train = combined_train)



# select the best model based upon
# the root mean squared error
xgb_best <- tune::select_best(
  xgb_results,
  metric = "rmse"
)

# cook up a model using finalize_workflow
# which takes workflow (model) specifications
# and combines it with optimal model
# parameters into a model workflow
xgb_best_hp <- tune::finalize_workflow(
  xgb_workflow,
  xgb_best
)






# train a final (best) model with optimal
# hyper-parameters
xgb_best_model <- fit(xgb_best_hp, combined_train)


source("../R/evaluation.R")
eval <- evaluate(combined_test,xgb_best_model)
 eval[[1]]
 eval[[2]]

 
```

## Neuronal Network

```{r}
 


#devtools::install_github("rstudio/tensorflow")
#devtools::install_github("rstudio/keras")
#Python version 3.11 needed (not 3.12!)!!!!!!
# tensorflow::install_tensorflow()
tensorflow::tf_config()
library(keras)
library(dplyr)









# Split the dataset into predictors and target variable
predictors_nn <- combined_train |>
  dplyr::select(all_of(predictors))
temperature <- combined_train |>
  dplyr::select(temperature)

# Normalize/Scale the predictors using dplyr
scaled_predictors <- predictors_nn %>%
  mutate(across(everything(), scale))

# Combining the scaled predictors with the temperature into a single data frame
processed_data <- cbind(scaled_predictors, temperature)

# Splitting the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(combined_train), 0.8 * nrow(combined_train))  # 80% for training
train_data <- processed_data[train_indices, ]
test_data <- processed_data[-train_indices, ]

# Separating predictors and target variable in training and testing sets
train_features <- train_data %>% select(-temperature)
train_labels <- train_data %>% pull(temperature)
test_features <- test_data %>% select(-temperature)
test_labels <- test_data %>% pull(temperature)

# Create a Keras sequential model (same as previous example)
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, activation = 'relu', input_shape = ncol(train_features)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1)


# Compile the model
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = keras$optimizers$legacy$Adam(learning_rate = 0.001),
  metrics = c('mean_absolute_error')
)

# Train the model
history <- model %>% fit(
  as.matrix(train_features), train_labels,
  epochs = 25,
  batch_size = 64,
  validation_data = list(as.matrix(test_features), test_labels)
)

# Evaluate the model
evaluation <- model %>% evaluate(as.matrix(test_features), test_labels)
cat("Mean Absolute Error on Test Data:", evaluation, "\n")

# Plot training history
plot(history)

# Make predictions
predictions <- model %>% predict(as.matrix(test_features))






```

```{r}




tiff_names <- str_sub(list.files("../data/Tiffs/"),end = -5)



tiffs_only <- terra::rast(paste0("../data/Tiffs/",tiff_names,".tif"))

if (model_demo == "y") {
  source("../R/shiny_map.R")
  shinyApp(ui = ui, server = server)
}else{
  source("../R/example_map.R")
}
  


  
```
