---
title: "main"
author: "Nils Tinner, Patrick Bigler"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
  '': default
editor_options:
  markdown:
    wrap: 75
---

## Preparation

First, we need some R-Packages. This code chunk install and load all
packages you need for reproduce this project. If you think you need another
package as well, then write it into the vector 'packages' and run the code
again.

```{r Load the Packages needed, message=FALSE, warning=FALSE, include=FALSE}
# Decide which packages you need. For this Project you need the following:
packages <- c("influxdbclient","ggplot2","tidyverse","lubridate","raster",
              "dplyr","googledrive","caret","rgdal","keras","vip","parsnip",
              "workflows","tune","dials","stringr","terra","stars","sf","plyr",
              "doParallel","terrainr","starsExtra", "pdp", "kableExtra", "recipes")

# Load the R script to install and load all the packages from above
source("../R/load_packages.R")
```

## Data Wrangling

In this section, the data...

### Raw TIF processing

What are we doing here and why?

### Data Combination

This code chunk creates the 'combined.csv' file which is the basis of the
whole project. It contains 10 geospatial data (6 land use classes and 4
geospatial layers)

```{r Data Wrangling, echo=FALSE, message=FALSE, warning=FALSE}
# We want to know, if a certain file already exists
name.of.file <- "../data/Combined.csv"

# If do not exists such a file, we create it
if (!file.exists(name.of.file)){
  # Load the R script to processing the raw tif files
  source("../R/raw_tif_processing_2.R")
  
  # Load the R script to create the data frame "combined" which is the basis of this project
  source("../R/data_combination.R")
  
  # We run the loaded function and drop all NAs
  data_combination() 
}#generate file
  combined <- read_csv("../data/Combined.csv") |>
    mutate(temperature = temperature-temp) |>
    drop_na()
  
  set.seed(123)
  loggers_test <- sample(unique(combined$Log_Nr), 10)

  combined_test <- combined |> 
    filter((Log_Nr %in% loggers_test))
  combined_train <- combined |> 
    filter(!(Log_Nr %in% loggers_test))

```

# Models

In this section, we define our predictors

## Model Definitions and Recipe

```{r Predictors and Recipe, message=FALSE, warning=FALSE, include=FALSE}

# Take all column-names you need as predictors from the combined file
predictors <- combined |>
  dplyr::select(-c(Log_Nr,temperature,timestamp,Name,NORD_CHTOP,OST_CHTOPO,year)) |>
  colnames()

# Define a formula in the following format: target ~ predictor_1 + ... + predictor_n
formula_local <- as.formula(paste("temperature","~", paste(predictors,collapse  = "+")))
  
# Make a recipe which can be used for the lm, KNN, and Random Forest model
pp <- recipes::recipe(formula_local,
                      data = combined_train) |>
    recipes::step_YeoJohnson(all_numeric(), -all_outcomes()) |> #extension of BoxCox? we will see wether that works...
    recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
    recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())
```

# Different Models

Here, you can select the model that will be used to eventually estimate the
map's spatial temperature distribution. You will find that the model with
the random forest approach shows the best performance, which is why it was
used for the map.

## Evaluation of the models

To fully implement the Open Sciences approach, the models can be computed
using the following code chunks. Further, for every model, a short synopsis
of the optimized hyperparameters is given. It must therefore be decided
before running the code chunk whether the hyperparameters determined by us
are to be used or whether the hyperparameters are to be optimized.

| Element of the list | What do you find                            |
|---------------------|---------------------------------------------|
| 1                   | A metric table (RSQ, RMSE, MAE and Bias)    |
| 2                   | Model visualization (Training and Test set) |
| 3                   |                                             |
| 4                   |                                             |

: **TABLE 1:** Overview of the function return of the evaluation function

| Model             | RSQ [Test set] | RMSE [Test set] | MAE [Test set] | Bias [Test set] |
|-----------------|---------------|---------------|---------------|---------------|
| Linear Regression | 0.14           | 1.01 °C         | 0.8 °C         | -0.04 °C        |
| KNN               |                |                 |                |                 |
| Random Forest     | 0.64           | 0.68 °C         | 0.51 °C        | -0.14 °C        |
| XGB Boost         |                |                 |                |                 |
| Neuronal Network  |                |                 |                |                 |

: **TABLE 2:** Overview of the evaluation of the models. The RMSE describes
the precision and the bias describes accuracy.

## LM Model

```{r Calculate a LM-Model, message=FALSE, warning=FALSE}
# Load the R script to calculate a lm model
source("../R/lm_model.R")
# The function needs the recipe and a dataset which can be used for model training
lm_model <- LM_Model(pp, combined_train)

source("../R/evaluation.R")
# The function will return a list
evaluation <- evaluation_function(combined_test, combined_train, lm_model)



# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# If we want a visualization of the training and test set
evaluation[[2]]
# VIP Plot
evaluation[[3]]

evaluation[[4]]

evaluation[[5]]


```

## KNN Model

The computational complexity is enormous and thus, KNN is not suitable for
very large data sets. You will need a very good device. We recommend using
our hyperparameter k = xx.

```{r KNN Model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to calculate a KNN model
source("../R/knn_model.R")

hyperpar.k <- c(5)
# knn_model <- KNN_Model(pp, combined_train, hyperpar.k)

# The function needs the recipe and a dataset which can be used for model training
knn_model <- KNN_Model(pp = pp, training_data =  combined_train, hyperpar.k = hyperpar.k)

# Call the function
result <- KNN_Model(pp = your_pp_value, 
                    training_data = your_training_data, 
                    hyperpar.k = hyperpar.k_values)

source("../R/evaluation.R")
evaluation <- evaluation_function(combined_test, combined_train, knn_model)
combined_test$fitted <- predict(knn_model, combined_test)
eval <- evaluate(combined_test,knn_model)
eval[[1]]
eval[[2]]
```

## Random Forest Model

Hyperparamater:

```{r Random Forest, echo=FALSE, message=FALSE, warning=FALSE}
source("../R/random_forest.R")
random_forest_model <- random_forest(pp,combined_train)

source("../R/evaluation.R")
evaluation <- evaluation_function(combined_test, combined_train, random_forest_model)

# If we want a table 
evaluation[[1]]
# Overview Training and Test
evaluation[[2]]
```

## XGB Model

Hyperparamater:

```{r}

source("../R/XGB.R")
# type of task we want to evaluate
model_settings <- parsnip::boost_tree(
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  stop_iter = 20,
) |>
  set_engine("xgboost",nthread = 6, verbose = T) |>
  set_mode("regression")
xgb_workflow <- workflows::workflow() |>
  add_formula(formula_local) |>
  add_model(model_settings)



xgb_results <- xgb(xgb_workflow,train = combined_train)



# select the best model based upon
# the root mean squared error
xgb_best <- tune::select_best(
  xgb_results,
  metric = "rmse"
)

# cook up a model using finalize_workflow
# which takes workflow (model) specifications
# and combines it with optimal model
# parameters into a model workflow
xgb_best_hp <- tune::finalize_workflow(
  xgb_workflow,
  xgb_best
)






# train a final (best) model with optimal
# hyper-parameters
xgb_best_model <- fit(xgb_best_hp, combined_train)


source("../R/evaluation.R")
eval <- evaluate(combined_test,xgb_best_model)
 eval[[1]]
 eval[[2]]

 
```

## Neuronal Network

Hyperparamater:

```{r}
 
predictors <- combined_train |>
  dplyr::select(-c(Log_Nr,temperature,timestamp,Name,NORD_CHTOP,OST_CHTOPO))
  
temperature <- combined_train$temperature
scaled_predictors <- scale(predictors)
 
   library(doParallel)
  cores <- detectCores()
  registerDoParallel(cores=detectCores())
 
 nn <- neuralnet(temperature ~., data = scaled_predictors, hidden = c(100, 60), linear.output = TRUE)
 
 plot(nn)
 
 
predicted_values <- compute(nn, scaled_predictors)$net.result





```



```{r}




tiff_names <- str_sub(list.files("../data/Tiffs/"),end = -5)


tiffs <- list()

for (tif in tiff_names) {
  tiffs[tif] = terra::rast(paste("../data/Tiffs/",tif,".tif",sep = ""))
  terra::set.names(tiffs[[tif]],tif)
}


meteoswiss <- read_csv2("../data/Meteoswiss/order_114596_data.txt")
meteoswiss_names <-meteoswiss|>
  dplyr::select(-stn,-time)|>
  colnames() 
# Goes:"gre000z0" "prestas0" "tre200s0" "rre150z0" "ure200s0" "fkl010z0" "dkl010z0"
#which is Globalstrahlung, Luftdruck, Lufttemperatur, Niederschlag, Relative Luftfeuchtigkeit, Windgeschwindigkeit, Windrichtung



meteo_pred <- c(1000,9559,25,0.0,600,2.3,206,12,8,14)
names(meteo_pred) <- c(meteoswiss_names[-1],"day","month","hour")



for (name in c(meteoswiss_names[-1],"day","month","hour")) {
  tiffs[[name]] <- terra::rast(ncol=293, nrow=247, xmin=2592670, xmax=2607320, ymin=1193202, ymax=1205552,names = name)
terra::values(tiffs[[name]]) <- unname(meteo_pred[name])

}




 minimal_extent <- terra::ext(tiffs[[1]])

# Loop through the list of raster paths to find the minimal extent
for (tif in tiffs) {
    raster_extent <- terra::ext(tif)
    minimal_extent <- terra::intersect(minimal_extent, raster_extent)
}
cropped <- terra::crop(tiffs[[1]],minimal_extent)


for (x in 2:length(tiffs)) {
tiffs[[x]]<- terra::crop(tiffs[[x]],minimal_extent)
}

tiffs_stack <- tiffs[[1]]

 for (x in 2:length(tiffs)) {
  tiffs_stack <- c(tiffs_stack,tiffs[[x]])
}

  temperature <- terra::predict(tiffs_stack,mod_cv,na.rm = T)
  
  
   extent <- rgdal::readOGR("../data/Map/Extent_Bern.shp")
  rivers <- rgdal::readOGR("../data/Map/Aare.shp")
  color = colorRampPalette(c("blue","deepskyblue", "white","orange", "red"))(100)
  raster::plot(temperature, col = color)
  sp::plot(extent, add = T)
  sp::plot(rivers, add = T)
  points( 2601930.3,1204410.1 , pch = 16, cex = 1)
  
```
