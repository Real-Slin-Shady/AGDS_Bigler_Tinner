---
title: "Workflow to Model the Urban Heat Island Effect of the City of Bern"
author: "Nils Tinner, Patrick Bigler"
date: "12-01-2023"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    latex_engine: xelatex
editor_options:
  markdown:
    wrap: 75
bibliography: references.bib
---

Course: Proseminar in Applied Geo-Data Science at the University of Bern
(Institute of Geography)

Supervisor: Prof. Dr. Benjamin Stocker

Adviser: Dr. Koen Hufkens, Dr. Laura Marques, Pepa Aran

Further information: <https://geco-bern.github.io/agds_proseminar/>

[Do you have questions about the workflow? Contact the
authors:]{.underline}

Tinner Nils (nils.tinner\@students.unibe.ch)

Bigler Patrick (patrick.bigler1\@students.unibe.ch)

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

# Introduction

Anthropogenic climate change is expected to increase the amount, intensity,
and duration of heat waves [@burger2021]. The urban heat island (UHI)
effect further amplifies this trend in urban environments [@burger2021],
[@gubler2021], [@wicki2018]. The UHI effect is expressed by higher air
temperatures in urban areas compared to rural areas in the region (Oke et
al. 2006). The effect is highest during night, as the emission of longwave
radiation in urban environments is impaired and sensible heat fluxes are
enhanced, whilst latent fluxes are reduced [@burger2021], [@gubler2021].
People living in urban areas are thus highly affected by the UHI effect via
thermal stress [@burger2021], [@wicki2018]). Given that more than 75% of
the Central European population lives in urban areas, the increasing trend
poses one of the major weather threats to people in urban environments
[@wicki2018]. Studying spatial temperature variabilities in urban areas is
therefore crucial to implement adaptation measures to minimize effects on
human health and the environment [@burger2021].

To capture small-scale temperature changes in these climatically complex
areas, high spatial resolution measurement networks are needed. However,
automated weather stations (AWS) are scarce due to their high costs. To
tackle this problem, @gubler2021 developed a new type of low-cost
measurement devices (LCDs). The LCD consists of a temperature logger and a
custom-made radiation shield that is naturally ventilated. 79 LCDs were
installed in the city of Bern, Switzerland in 2018 @gubler2021. In
@gubler2021 reported an overestimation of hourly mean temperature
measurements by the LCDs (0.61 °C - 0.93 °C) compared to the reference
station (AWS) during daytime (06:00 -- 22:00). During night-time (22:00 --
06:00), differences were much lower or even negative (between -0.12 °C and
0.23 °C). But not only the LCD temperature and the anomaly between the LCDs
and the AWS is interesting, but also the temperature distribution of the
entire region of Bern, shown on a map.

In spring 2023, Nils Tinner has the idea to generate a map of the
distribution of temperatures in Bern for the current temperatures of the
LCDs. He improved the map in June 2023 by using geospatial data from the
study of @burger2021 . The map predicted the local temperature in the city
of Bern based on those geospatial data and was based on @burger2021 . The
approach was a classic multivariate linear regression model. As the model
only knows the current temperature distribution, its scope and statistical
power are limited. Both @burger2021 and @gubler2021 showed, that the city
of Bern is affected by the urban heat island effect. This fact and the
limited power of the first map from Nils Tinner motivated us to investigate
the urban heat island once again.

## Objectives and Research Questions

Because @burger2021 and @gubler2021 only used a multiple linear regression
approach to model the urban heat island effect in the city of Bern, this
study aims to calculate our own models by using a classic approach
(multiple linear regression model) and machine learning approaches
(k-nearest neigbors (knn) and random forest). For model calculations, we
use partial the same data as @burger2021 but note that this study is not a
redoing of the study of @burger2021 . After the model calculation we
compare our models with each other and use the best to model and
investigate the Urban Heat Island effect of the city of Bern. At last, we
want to visualize our findings in a map. This leads to our four research
questions:

1.  Are the two machine learning approaches (knn and random forest)
    reasonable approaches for modeling the urban heat island effect in the
    city of Bern in terms of explaining the variance ($R^2$), the precision
    (RMSE) and the accuracy (bias)?
2.  Are the machine learning approaches superior to a multiple regression
    model in terms of explaining the variance ($R^2$), the precision (RMSE)
    and the accuracy (bias)?
3.  Can a meaningful map be generated from the best model that shows the
    spatial distribution of the temperature anomaly?
4.  What can be said from this modeling about the modeling of @burger2021
    in terms of explaining the variance ($R^2$), the precision (RMSE) and
    the accuracy (bias)?

However, this study does not aim to provide a new and reliable method for
modeling the UHI effect in the city of Bern. Nevertheless, it is intended
to show that the machine learning approaches examined are promising and
that the full potential has not yet been reached.

## How this Workflow Works

In the Data and Methodology section you will see that we work with a large
amount of data. This means the computational time could be very high. In
order to we implemented different workflows within this markdown. If you
change the input of the functions in the first code chunk, you will run the
workflow in a different mode. Figure 1 gives you a overview about the
default mode and all other options you have:

SCHEMATIC OF THE DIFFERENT WORKFLOWS

Please note that the default version is only guaranteed until the end of
February 2024. After that, you have to change the input ofl function xx
from TRUE to FALSE.

```{r Choose your workflow mode}



```

# Data and Methodology

## Data

| Variable                                       | Variable Typ     | Resolution / [used]    |
|-----------------------------------|--------------------|--------------------|
| 2m Temperature in °C [Zollikofen]              | Meteorological   | 15 minutes / [1h mean] |
| Precipitation in mm [Zollikofen]               | Meteorological   | 15 minutes / [1h mean] |
| Radiation in $W*m^{-2}$ [Zollikofen]           | Meteorological   | 15 minutes / [1h mean] |
| Windspeed in $m*s^{-1}$ [Zollikofen]           | Meteorological   | 15 minutes / [1h mean] |
| Winddirection in ° [Zollikofen]                | Meteorological   | 15 minutes / [1h mean] |
| Buildings                                      | Land use class   | [25m / 150m / 1000m]   |
| Open Space Forest                              | Land use class   | [25m / 150m / 1000m]   |
| Open Space Garden                              | Land use class   | [25m / 150m / 1000m]   |
| Open Space Sealed                              | Land use class   | [25m / 150m / 1000m]   |
| Open Space Agriculture                         | Land use class   | [25m / 150m / 1000m]   |
| Open Space Water                               | Land use clas    | [25m / 150m / 1000m]   |
| Vegetation Height                              | Geospatial Layer | [25m / 150m / 1000m]   |
| Mean Building Heights                          | Geospatial Layer | [25m / 150m / 1000m]   |
| Slope                                          | Geospatial Layer | [25m / 150m / 1000m]   |
| Digital Elevation Model (DEM)                  | Geospatial Layer | [25m / 150m / 1000m]   |
| Climate Network in °C (\~120 low cost devices) | Meteorological   | 10 minutes / [1h mean] |

: **TABLE 1:** Overview of the predictors used for modeling.

To evaluate...LCDs...target?.......Because we also want to show a possible
application of such models the best model will be used to show spatial
temperature distributions for a possible day as a map.

# Data Wrangling

In this section, all preparations for modeling are performed. This first
subsection ensures that you have all required packages. The second
subsection aims to generate the basis data frame (Combined.csv) in a
interactive way.

## Packages

This code chunk install and load all packages you need for reproduce this
project. If you think you need another package as well, then write it into
the vector 'packages' and run the code again.

```{r Load the Packages needed, message=FALSE, warning=FALSE, include=FALSE}
# Decide which packages you need. For this Project you need the following:
packages <- c("influxdbclient","ggplot2","tidyverse","lubridate","raster",
              "dplyr","googledrive","caret","rgdal","keras","vip","parsnip",
              "workflows","tune","dials","stringr","terra","stars","sf","plyr",
              "doParallel", "foreach", "terrainr","starsExtra", "pdp", "recipes", 
              "tidyterra","shiny", "xgboost", 'kableExtra')

# Load the R script to install and load all the packages from above
source("../R/load_packages.R")
```

## Choose your Workflow

This subsection generate your basic .csv file (it is called 'sCombined') in
a interactive way. Therefore, you have to navigate through some questions.
First, it will check whether a Combined.csv file already exists in the
corresponding folder. If it, then it will read in. If not, then three
questions will follow. Here, you can see those questions and an explanation
what the answers mean for the further workflow:

1.  You have a Combined.csv file already. Would you like to redo the
    proceeding?

    If you answer with 'yes', then will guided through the following
    questions and your Combined.csv file will be rewritten. If you answer
    with 'no', then it will read your existing file and you will directly
    jump to the model section.

2.  Do you want to generate all data by yourself? (It may takes up to 12
    hours)

    If you answer with 'yes', all data will be downloaded and processed
    again. It may takes several hours. If you answer with 'no', then you
    will be asked a second question:

3.  Do you want to proceed in a demo version?

    If you answer with 'yes', it will download all data from a dropbox
    account and generate a .csv file which contains 5 % of the data. With
    this option, all calculations can be done in max. 5 minutes. If you
    answer with 'no', then it will download all data from a dropbox account
    and generate a .csv file which contains all data. With this option, the
    duration for some calculations can exceed 30'.

```{r Data Wrangling, echo=FALSE, message=FALSE, warning=FALSE}
knit <- "y" #CHANGE HERE FOR EXPLORATION OF REPORT IN R

if (knit == "y") {
  model_demo <- "y"
  advanced_models <- "n"

source("../R/demo_download.R") #directly download
source("../R/data_combination.R") #and process
data_combination()
}else{
  # Load the R script to start the workflow
source("../R/Processing_Brain.R")
preprocessing()

source("../R/model_training_brain.R")
model_training_brain()
}


  combined <- read_csv("../data/Combined.csv") |>
    mutate(temperature = temperature-temp) |>
    drop_na()

  if (model_demo == "y") {
    # choose 5% of the data randomly
    combined <- dplyr::slice_sample(combined,prop = .05) 
  }

# Set seed for reproducibility
set.seed(123)

# (pseudo) random sample
loggers_test <- sample(unique(combined$Log_Nr), 10)

# Generate a test set
combined_test <- combined |> 
  filter((Log_Nr %in% loggers_test))

# Generate a training set
combined_train <- combined |> 
  filter(!(Log_Nr %in% loggers_test))
```

# Preparations for Modelling

This section provides information about the preparation for modelling. The
first subsection gives a overview about the variable selection. The second
subsection provides information about the model recipe.

## Variable Selection

First, a variable selection was carried out (R-script: Processing_Brain.R).
Table two provides a brief overview of why certain variables were rejected.
Further details can be found in the R-script mentioned above.Based on this
selection, a formula is now generated, which is processed into a recipe.
This is now the basis for creating the models.

| Variable which is not a predictor | Reason                                                                          |
|--------------------------|-------------------------------------------------|
| Log_Nr                            |                                                                                 |
| temperature                       | because this is the target                                                      |
| timestamp, year, month, day, hour | because it is controversial whether time and date should be used as predictors. |
| NORD_CHTOP, OST_CHTOP             | Coordinates of                                                                  |
| LV_03_E, LV_03_N                  | Coordinates of                                                                  |

: **TABLE 2:** Overview about the reasons why a variable/column were
rejected as a predictor

## Model Recipe

```{r Predictors and Recipe, echo=FALSE, message=FALSE, warning=FALSE}
# Take all column-names you need as predictors from the combined file
predictors <- combined |>
  # select our predictors (we want all columns except those in the select() function)
  dplyr::select(-c(Log_Nr,temperature,timestamp,Name,NORD_CHTOP,OST_CHTOPO,
                   year,month,day,hour,LV_03_E,LV_03_N)) |>
  colnames()

# Define a formula in the following format: target ~ predictor_1 + ... + predictor_n
formula_local <- as.formula(paste("temperature","~", paste(predictors,collapse  = "+")))
  
# Make a recipe which can be used for the lm, KNN, and Random Forest model
pp <- recipes::recipe(formula_local,
                      data = combined_train) |>
    # Yeo-Johnsen transformation (includes Box Cox and an extansion. Now it can handle x ≤ 0)
    recipes::step_YeoJohnson(all_numeric(), -all_outcomes()) |> 
    # subsracting the mean from each observation/measurement
    recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
    # transforming numeric variables to a similar scale
    recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())
```

# Modelling

Here, you can select the model that will be used to eventually estimate the
map's spatial temperature distribution. You will find that the model with
the random forest approach shows the best performance, which is why it was
used for the map.

## Overview about the Evaluation of the Models

| Model             | RSQ [Test set] | RMSE [Test set] | MAE [Test set] | Bias [Test set] |
|---------------|---------------|---------------|---------------|---------------|
| Linear Regression |                |                 |                |                 |
| KNN               |                |                 |                |                 |
| Random Forest     |                |                 |                |                 |
| XGB Boost         | \-             | \-              | \-             | \-              |
| Neuronal Network  | \-             | \-              | \-             | \-              |

: **TABLE 4:** Overview of the evaluation of the models with data from
Burger et al. (2019).

To fully implement the Open Sciences approach, the models can be computed
using the following code chunks. Further, for every model, a short synopsis
of the optimized hyperparameters is given. It must therefore be decided
before running the code chunk whether the hyperparameters determined by us
are to be used or whether the hyperparameters are to be optimized.

| Element of the list | What do you find                                                                               |
|----------------------|-----------------------------------------------------|
| 1                   | returns a table with the main metrics (RSQ, RMSE, MAE and Bias)                                |
| 2                   | retunrs a visualization of the model (Training and Test set) and shows RSQ, RMSE and the bias. |
| 3                   | returns a boxplot for each logger station (24h)                                                |
| 4                   | returns a boxplot for each hour of the day                                                     |

: **TABLE 2:** Overview of the returned list of the evaluation function

The aim of this project is the comparison of machine learning techniques to
a linear regression. Further, we want to explore the superior of machine
learning compared to a linear regression. Table 3 gives you an overview
about our results, if you use the entire 'combined' data set. You see, that
both KNN and random forest are superior compared to the linear regression
model.

The random forest model explains about 73% of the variance. The RSME is
about 0.63 °C. Note that the error of a single LCD in the climate network
of the city of Bern is about 1.5°C (see Burger et all 2019). Further we can
see, that data scattering is about 0.2°C lower than for a KNN- or lm-model
(accuracy). The bias is for all models about 0.07°C (systematic error /
precision). It is positive and therefore our models overestimate the
temperature (see evaluation and access the second element of the list).

The XGB Boost and the neuronal network are additional and should show you,
that there are further techniques which could be explored. But those model
we did not taken into account.

| Model                | RSQ [Test set] | RMSE [Test set] | MAE [Test set] | Bias [Test set] |
|---------------|---------------|---------------|---------------|---------------|
| Linear Regression    | 0.47           | 0.86 °C         | 0.67 °C        | 0.06 °C         |
| KNN [reduced to 13%] | 0.51           | 0.84 °C         | 0.65 °C        | 0.07 °          |
| Random Forest        | 0.73           | 0.63 °C         | 0.46 °C        | 0.07 °C         |
| XGB Boost            | \-             | \-              | \-             | \-              |
| Neuronal Network     | \-             | \-              | \-             | \-              |

: **TABLE 3:** Overview of the evaluation of the models in the regular
mode.

As mentioned, the model calculation can be very time intensive. Table 4
shows the results for approach with reduced data. Overall we can say, that
the models perform less. All values are lesser than in table 3. Note the
high bias for the random forest. The model highli overestimate the
temperature. We do not have an explanation for this. May be there is a
problem with the reducing process. If we only use 5% of our data, then we
lose a lot of information. If we lose all information about a certain
logger station but the model predict this station, the it could b that the
model perfoms worse. But we are not sure and more investigations are
needed. But as a conclusion we can say, that more data improve the models.

| Model             | RSQ [Test set] | RMSE [Test set] | MAE [Test set] | Bias [Test set] |
|---------------|---------------|---------------|---------------|---------------|
| Linear Regression | 0.47           | 0.96 °C         | 0.75 °C        | 0.09 °C         |
| KNN               | 0.58           | 0.86 °C         | 0.63 °C        | 0.06 °C         |
| Random Forest     | 0.64           | 0.84 °C         | 0.59 °C        | 0.21 °C         |
| XGB Boost         | \-             | \-              | \-             | \-              |
| Neuronal Network  | \-             | \-              | \-             | \-              |

: **TABLE 4:** Overview of the evaluation of the models in the demo mode.

## Linear Regression Model

### Model

The lm-model is a classic multiple linear regression model. The function
need a recipe (pp) and train data as inputs. There is no possibility for
tuning the model.

```{r Calculate lm-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to calculate a lm model
source("../R/lm_model.R")

# The function needs the recipe and a dataset which can be used for model training
lm_model <- LM_Model(pp, combined_train)
```

### Evaluation

Here we evaluate the lm-model. We use our evaluate_function(). The return
will be a list; we can choose which part of the evaluation we want access
to (see table 2).

```{r Evaluation of the lm-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to evaluate the model
source("../R/evaluation.R")
# The function will return a list
evaluation <- evaluation_function(combined_test, combined_train, lm_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# If we want a visualization of the training and test set
evaluation[[2]]
# Boxplot for each logger station (24h)
evaluation[[3]]
# Boxplot for each hour of the day
evaluation[[4]]
```

## KNN Model

The KNN-model can be very time intensive. We use the DoParallel Package for
that. Please make sure, that your device do not anything else.

### Model

The KNN-model uses the recipe (pp) and training data as inputs. The
function calculate a KNN-Model with the recipe and uses 3 cross validations
by logger numbers. Further, you have the option for tuning the
hyper-parameter k by using tuning=TRUE/FALSE. Note that this can be very
time intensive. We have done it already and the function will use k = 10 as
a default. If you want to do it again, change tuning = TRUE and the
function will tune the model for k = c(8, 9, 10, 11, 12). Moreover, the
function also has a safety feature implemented. Because the computing time
for a KNN-model can be very high, the data input is limited to 100,000
rows. The adjustment takes place automatically.

```{r Calculation of the knn-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to calculate a KNN model
source("../R/knn_model.R")

# The function needs the recipe and a dataset which can be used for model training
knn_model <- KNN_Model(pp = pp, training_data =  combined_train, tuning = FALSE)
```

### Evaluation

Here we evaluate the KNN-model. We use our evaluate_function() again. The
return will be a list; we can choose which part of the evaluation we want
access to (see table 2).

```{r Evaluation of the knn-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to evaluate the model
source("../R/evaluation.R")

# Function call
evaluation <- evaluation_function(combined_test, combined_train, knn_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# If we want a visualization of the training and test set
evaluation[[2]]
# Boxplot for each logger station
evaluation[[3]]
# Boxplot for each hour of the day (24h)
evaluation[[4]]
```

## Random Forest Model

The random forest-model can be very time intensive. We use the DoParallel
Package for that. Please make sure, that your device do not anything else.

### Model

The function calculate a random forest-model with the recipe and uses 3
cross validations by logger numbers. Further, you have the option for
tuning the hyper-parameters by using tuning=TRUE/FALSE. Note that this can
be very time intensive. We have done it already and the function will use
mtry = number of predictors/3 and min.node.size = 5 as default.

```{r Calculation of the random forest-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to calculate a random forest model
source("../R/random_forest.R")

# The function needs the recipe and a dataset which can be used for model training
random_forest_model <- random_forest(pp, combined_train, tuning = F)
```

### Evaluation

Here we evaluate the random forest-model. We use our evaluate_function()
again. The return will be a list; we can choose which part of the
evaluation we want access to (see table 2).

```{r Evaluation of the random forest-model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to evaluate the model
source("../R/evaluation.R")

# Function call
evaluation <- evaluation_function(combined_test, combined_train, random_forest_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# If we want a visualization of the training and test set
evaluation[[2]]
# Boxplot for each logger station (24h)
evaluation[[3]]
# Boxplot for each hour of the day
evaluation[[4]]
```

## XGB Model

### Model

Hyperparamater:

```{r}


if(advanced_models == "y"){
source("../R/XGB.R")
# type of task we want to evaluate


xgb_model <- xgb(data_train = combined_train,formula = formula_local) #this model always tunes...

source("../R/evaluation.R")

# Function call
evaluation <- evaluation_function(combined_test, combined_train, xgb_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# If we want a visualization of the training and test set
evaluation[[2]]
# Boxplot for each logger station (24h)
evaluation[[3]]
# Boxplot for each hour of the day
evaluation[[4]]


}


```

## Neuronal Network

### Model

```{r}
 

if(advanced_models == "y"){
  source("../R/neural_network.R")
  #tensorflow::install_tensorflow() may need to be run...
  nn_model <- neural_network(combined_train)
#preprocessing needed for test imput of nn
  predictors_nn <- combined_test |>
  dplyr::select(all_of(predictors))
temperature <- combined_test |>
  dplyr::select(temperature)

# Normalize/Scale the predictors using dplyr
scaled_predictors <- predictors_nn|>
  mutate(across(everything(), scale))

# Combining the scaled predictors with the temperature into a single data frame
processed_data <- cbind(scaled_predictors, temperature)




test_data <- processed_data

  
test_features <- test_data|> select(-temperature)
test_labels <- test_data|> pull(temperature)
  
  
  
  # Evaluate the model
evaluation <- nn_model|> evaluate(as.matrix(test_features), test_labels)
cat("Mean Absolute Error on Test Data:", evaluation, "\n")

# Make predictions
combined_test$nn_prediction <- nn_model|> predict(as.matrix(test_features))


cor(combined_test$nn_prediction,combined_test$temperature)
  


}
```

### Evaluation

# Map

```{r}
source('../R/example_map.R')

```



# Discussion

# Bibliography

[@burger2021]

[@gubler2021]

[@stocker2023]

[@wicki2018]

\
Oke, T. (2006), Initial Guidance to Obtain Representative Meteorological
Observations at\
Urban Sites: Instruments and Observing Methods. IOM Report No. 81., Canada:
World\
Meteorological Organization

\
Oke, T. R., Mills, G., Christen, A. and Voogt, J. A. (2017), Concepts. In:
Urban Climates,\
Cambridge University Press, p. 14--43
