---
title: "Main Workflow"
author: "Nils Tinner, Patrick Bigler"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
  '': default
editor_options:
  markdown:
    wrap: 75
---

Course: Proseminar in Applied Geo-Data Science at the University of Bern
(Institute of Geography)

Supervisor: Prof. Dr. Benjamin Stocker

Adviser: Dr. Koen Hufkens, Dr. Laura Marques, Pepa Aran

Further information: <https://geco-bern.github.io/agds_proseminar/>

[Do you have questions about the workflow? Contact the
authors:]{.underline}

Tinner Nils (nils.tinner\@students.unibe.ch)

Bigler Patrick (patrick.bigler1\@students.unibe.ch)

\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--

Welcome to our reproducible workflow for the proseminar in Applied Geodata
Sciences project. This project aims to compare different machine learning
techniques to a linear regression model and explores whether machine
learning approaches are superior to linear regression models. For this
purpose, the temperature deviation of the city of Bern compared to the
MeteoSwiss measuring station in Zollikofen is modeled (urban heat island
effect). For modeling, we use 5 meteorological variables from the
MeteoSchweiz measuring station in Zollikofen for the time period 2019-2022,
6 land use classes and 4 geospatial layers. The land use classes and the
geospatial layers covers the entire suburban area of the city of Bern.

| Variable                      | Variable Typ     | Resolution / [used] |
|-------------------------------|------------------|---------------------|
| 2m Temperature [Zollikofen]   | Meteorological   | 15 minutes / [1h]   |
| Precipitation [Zollikofen]    | Meteorological   | 15 minutes / [1h]   |
| Radiation [Zollikofen]        | Meteorological   | 15 minutes / [1h]   |
| Windspeed [Zollikofen]        | Meteorological   | 15 minutes / [1h]   |
| Winddirection [Zollikofen]    | Meteorological   | 15 minutes / [1h]   |
| Buildings                     | Land use class   |                     |
| Open Space Forest             | Land use class   |                     |
| Open Space Garden             | Land use class   |                     |
| Open Space Sealed             | Land use class   |                     |
| Open Space Agriculture        | Land use class   |                     |
| Open Space Water              | Land use clas    |                     |
| Vegetation Height             | Geospatial Layer |                     |
| Mean Building Heights         | Geospatial Layer |                     |
| Slope                         | Geospatial Layer |                     |
| Digital Elevation Model (DEM) | Geospatial Layer |                     |

: **TABLE 1:** Overview of the predictors used for modeling. For more
information please read the [project
proposal](https://github.com/sundin01/AGDS_Bigler_Tinner/blob/main/proposal.pdf).

To evaluate...LCDs...target?.......Because we also want to show a possible
application of such models the best model will be used to show spatial
temperature distributions for a possible day as a map.

The computing time is very high because the workflow processes a large
amount of data. We have therefore designed the workflow interface to be
interactive. The interface is built according to the following graph:

SCHEMATIC OF THE INTERFACE. We could do that with a GGPLOT geom_rect
layers....

Please note that the demo version is only guaranteed until the end of
February 2024. After that, the layers must be regenerated, which takes
several hours (depends on your device).

# Preparation

In this section, all preparations for modeling are performed. This first
subsection ensures that you have all required packages. The second
subsection aims to generate the basis data frame (Combined.csv) in a
interactive way.

## Packages

This code chunk install and load all packages you need for reproduce this
project. If you think you need another package as well, then write it into
the vector 'packages' and run the code again.

```{r Load the Packages needed, message=FALSE, warning=FALSE, include=FALSE}
# Decide which packages you need. For this Project you need the following:
packages <- c("influxdbclient","ggplot2","tidyverse","lubridate","raster",
              "dplyr","googledrive","caret","rgdal","keras","vip","parsnip",
              "workflows","tune","dials","stringr","terra","stars","sf","plyr",
              "doParallel", "foreach", "terrainr","starsExtra", "pdp",
              "kableExtra", "recipes", "tidyterra","shiny")

# Load the R script to install and load all the packages from above
source("../R/load_packages.R")
```

## Data Wrangling

This subsection generate your basic csv. file (Combined.csv) in a
interactive way.

1.  You have a Combined.csv file already. Would you like to redo the
    proceeding?

    If you answer with 'yes', then will guided through the following
    questions and your Combined.csv file will be rewritten. If you answer
    with 'no', then you will jump to the model section.

2.  Do you want to generate all data by yourself? (It may takes up to 12
    hours)

    If you answer with 'yes', all data will be downloaded and processed
    again. It may takes several hours. If you answer with 'no', then you
    will be asked a second question:

3.  Do you want to proceed in a demo version?

    If you answer with 'yes', it will download all data from a dropbox
    account and generate a .csv file which contains 5 % of the data. With
    this option, all calculations can be done in max. 5 minutes. If you
    answer with 'no', then it will download all data from a dropbox account
    and generate a .csv file which contains all data. With this option, the
    duration for some calculations can exceed 30'.

```{r Data Wrangling, echo=FALSE, message=FALSE, warning=FALSE}

source("../R/Processing_Brain.R")

repeat{ print("The normal version of model Training takes a while.")
model_demo = readline(prompt = "Would you like to run the model training in demo mode? [y/n] ")
  if(model_demo %in% c("y","n")){break}
  }

  combined <- read_csv("../data/Combined.csv") |>
    mutate(temperature = temperature-temp) |>
    drop_na()

  if (model_demo == "y") {
    combined <- dplyr::slice_sample(combined,prop = .05) 
  }
  
set.seed(123)
loggers_test <- sample(unique(combined$Log_Nr), 10)

combined_test <- combined |> 
  filter((Log_Nr %in% loggers_test))

combined_train <- combined |> 
  filter(!(Log_Nr %in% loggers_test))
```

# Preparations for Modelling

This section provides information about the preparation for modelling. The
first subsection gives a overview about the variable selection. The second
subsection provides information about the model recipe.

## Variable Selection

First, a variable selection was carried out (R-script: Processing_Brain.R).
Table two provides a brief overview of why certain variables were rejected.
Further details can be found in the R-script mentioned above.Based on this
selection, a formula is now generated, which is processed into a recipe.
This is now the basis for creating the models.

| Variable which is not a predictor | Reason                                                                          |
|-----------------------|----------------------------------------------------|
| Log_Nr                            |                                                                                 |
| temperature                       | because this is the target                                                      |
| timestamp, year, month, day, hour | because it is controversial whether time and date should be used as predictors. |
| NORD_CHTOP, OST_CHTOP             | Coordinates of                                                                  |
| LV_03_E, LV_03_N                  | Coordinates of                                                                  |

: **TABLE 2:** Overview about the reasons why a variable/column were
rejected as a predictor

## Model Recipe

```{r Predictors and Recipe, message=FALSE, warning=FALSE, include=FALSE}
# Take all column-names you need as predictors from the combined file
predictors <- combined |>
  dplyr::select(-c(Log_Nr,temperature,timestamp,Name,NORD_CHTOP,OST_CHTOPO,
                   year,month,day,hour,LV_03_E,LV_03_N)) |>
  colnames()

# Define a formula in the following format: target ~ predictor_1 + ... + predictor_n
formula_local <- as.formula(paste("temperature","~", paste(predictors,collapse  = "+")))
  
# Make a recipe which can be used for the lm, KNN, and Random Forest model
pp <- recipes::recipe(formula_local,
                      data = combined_train) |>
    recipes::step_YeoJohnson(all_numeric(), -all_outcomes()) |> 
    recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
    recipes::step_scale(recipes::all_numeric(), -recipes::all_outcomes())
```

# Modelling

Here, you can select the model that will be used to eventually estimate the
map's spatial temperature distribution. You will find that the model with
the random forest approach shows the best performance, which is why it was
used for the map.

## Overview about the Evaluation of the Models

To fully implement the Open Sciences approach, the models can be computed
using the following code chunks. Further, for every model, a short synopsis
of the optimized hyperparameters is given. It must therefore be decided
before running the code chunk whether the hyperparameters determined by us
are to be used or whether the hyperparameters are to be optimized.

| Element of the list | What do you find                            |
|---------------------|---------------------------------------------|
| 1                   | A metric table (RSQ, RMSE, MAE and Bias)    |
| 2                   | Model visualization (Training and Test set) |
| 3                   | Boxplot for each logger station (24h)       |
| 4                   | Boxplot for each hour of the day            |
| 5                   | Partial dependence of the variables         |

: **TABLE 2:** Overview of the function return of the evaluation function

dedd

| Model                | RSQ [Test set] | RMSE [Test set] | MAE [Test set] | Bias [Test set] |
|------------------|--------------|---------------|--------------|---------------|
| Linear Regression    | 0.47           | 0.86 °C         | 0.67 °C        | 0.02 °C         |
| KNN [reduced to 13%] | 0.51           | 0.84 °C         | 0.65 °C        | 0.07 °          |
| Random Forest        | 0.73           | 0.63 °C         | 0.46 °C        | 0.07 °C         |
| XGB Boost            |                |                 |                |                 |
| Neuronal Network     |                |                 |                |                 |

: **TABLE 3:** Overview of the evaluation of the models in the regular
mode.

dedferfef

| Model             | RSQ [Test set] | RMSE [Test set] | MAE [Test set] | Bias [Test set] |
|---------------|---------------|---------------|---------------|---------------|
| Linear Regression | 0.32           | 1.00 °C         | 0.79 °C        | 0.05 °C         |
| KNN               | 0.45           | 0.91 °C         | 0.71 °C        | 0.1 °C          |
| Random Forest     | 0.54           | 0.8 °C          | 0.6 °C         | 0.01 °C         |
| XGB Boost         |                |                 |                |                 |
| Neuronal Network  |                |                 |                |                 |

: **TABLE 4:** Overview of the evaluation of the models in the demo mode.

## Linear Regression Model

### Model

```{r Calculate a LM-Model, message=FALSE, warning=FALSE}
# Load the R script to calculate a lm model
source("../R/lm_model.R")
# The function needs the recipe and a dataset which can be used for model training
lm_model <- LM_Model(pp, combined_train)
```

### Evaluation

```{r}
# Load the R script to evaluate the model
source("../R/evaluation.R")
# The function will return a list
evaluation <- evaluation_function(combined_test, combined_train, lm_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# If we want a visualization of the training and test set
evaluation[[2]]
# Boxplot for each logger station (24h)
evaluation[[3]]
# Boxplot for each hour of the day
evaluation[[4]]
```

## KNN Model

### Model

The KNN model uses the recipe (pp) and training data as inputs. Further,
you have the option for tuning the hyperparameter k. Note that this can be
very time intensive. We have done it already and the function will use k =
10 as a default. If you change tuning = TRUE, the function will tune the
model.

Furthermore, a KNN model is not really suitable for large data sets. The
computation time is very high. y

```{r KNN Model, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to calculate a KNN model
source("../R/knn_model.R")
# The function needs the recipe and a dataset which can be used for model training
knn_model <- KNN_Model(pp = pp, training_data =  combined_train, tuning = FALSE)
```

### Evaluation

```{r}
# Load the R script to evaluate the model
source("../R/evaluation.R")

# Function call
evaluation <- evaluation_function(combined_test, combined_train, knn_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# If we want a visualization of the training and test set
evaluation[[2]]
# Boxplot for each logger station (24h)
evaluation[[3]]
# Boxplot for each hour of the day
evaluation[[4]]
```

## Random Forest Model

### Model

Hyperparamater:

```{r Random Forest, echo=FALSE, message=FALSE, warning=FALSE}
# Load the R script to calculate a random forest model
source("../R/random_forest.R")
# The function needs the recipe and a dataset which can be used for model training
random_forest_model <- random_forest(pp, combined_train, tuning = F)
```

### Evaluation

```{r}
# Load the R script to evaluate the model
source("../R/evaluation.R")

# Function call
evaluation <- evaluation_function(combined_test, combined_train, random_forest_model)

# If we want our metrics in table, we chose the first element of the list
evaluation[[1]]
# If we want a visualization of the training and test set
evaluation[[2]]
# Boxplot for each logger station (24h)
evaluation[[3]]
# Boxplot for each hour of the day
evaluation[[4]]
```

## XGB Model

### Model

Hyperparamater:

```{r}

source("../R/XGB.R")
# type of task we want to evaluate
model_settings <- parsnip::boost_tree(
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  stop_iter = 20,
) |>
  set_engine("xgboost",nthread = 6, verbose = T) |>
  set_mode("regression")
xgb_workflow <- workflows::workflow() |>
  add_formula(formula_local) |>
  add_model(model_settings)



xgb_results <- xgb(xgb_workflow,train = combined_train)



# select the best model based upon
# the root mean squared error
xgb_best <- tune::select_best(
  xgb_results,
  metric = "rmse"
)

# cook up a model using finalize_workflow
# which takes workflow (model) specifications
# and combines it with optimal model
# parameters into a model workflow
xgb_best_hp <- tune::finalize_workflow(
  xgb_workflow,
  xgb_best
)
# train a final (best) model with optimal
# hyper-parameters
xgb_best_model <- fit(xgb_best_hp, combined_train)

```

### Evaluation

```{r}
source("../R/evaluation.R")
eval <- evaluate(combined_test,xgb_best_model)
 eval[[1]]
 eval[[2]]

```

## Neuronal Network

### Model

```{r}
 


#devtools::install_github("rstudio/tensorflow")
#devtools::install_github("rstudio/keras")
#Python version 3.11 needed (not 3.12!)!!!!!!
# tensorflow::install_tensorflow()
tensorflow::tf_config()
library(keras)
library(dplyr)

# Split the dataset into predictors and target variable
predictors_nn <- combined_train |>
  dplyr::select(all_of(predictors))
temperature <- combined_train |>
  dplyr::select(temperature)

# Normalize/Scale the predictors using dplyr
scaled_predictors <- predictors_nn %>%
  mutate(across(everything(), scale))

# Combining the scaled predictors with the temperature into a single data frame
processed_data <- cbind(scaled_predictors, temperature)

# Splitting the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(1:nrow(combined_train), 0.8 * nrow(combined_train))  # 80% for training
train_data <- processed_data[train_indices, ]
test_data <- processed_data[-train_indices, ]

# Separating predictors and target variable in training and testing sets
train_features <- train_data %>% select(-temperature)
train_labels <- train_data %>% pull(temperature)
test_features <- test_data %>% select(-temperature)
test_labels <- test_data %>% pull(temperature)

# Create a Keras sequential model (same as previous example)
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, activation = 'relu', input_shape = ncol(train_features)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1)


# Compile the model
model %>% compile(
  loss = 'mean_squared_error',
  optimizer = keras$optimizers$legacy$Adam(learning_rate = 0.001),
  metrics = c('mean_absolute_error')
)

# Train the model
history <- model %>% fit(
  as.matrix(train_features), train_labels,
  epochs = 25,
  batch_size = 64,
  validation_data = list(as.matrix(test_features), test_labels)
)

# Evaluate the model
evaluation <- model %>% evaluate(as.matrix(test_features), test_labels)
cat("Mean Absolute Error on Test Data:", evaluation, "\n")

# Plot training history
plot(history)

# Make predictions
predictions <- model %>% predict(as.matrix(test_features))
```

### Evaluation

# Map

# Shiny App

```{r}

tiff_names <- str_sub(list.files("../data/Tiffs/"),end = -5)

tiffs_only <- terra::rast(paste0("../data/Tiffs/",tiff_names,".tif"))

if (model_demo == "y") {
  source("../R/shiny_map.R")
  shinyApp(ui = ui, server = server)
}else{
  source("../R/example_map.R")
}
```
